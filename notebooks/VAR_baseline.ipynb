{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUPZrxEAwDCNmtSIQah8ox"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Intro etc"
      ],
      "metadata": {
        "id": "QSQilOn5kG5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "...\n",
        "\n",
        "TODO Summarise results\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Comparison with Baselines\n",
        "\n",
        "Finally, we can compare the best performing gradient boosted etc models against the best baseline method.  The VAR (Vector Auto-Regression) model from the [baselines notebook](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/cammet_baselines_2021.ipynb) was the best performing baseline.\n",
        "\n",
        "The best encoder decoder model, after 5 training epochs, was conv2dk2d_28l_48s_16bs_448fm_64f_1ksf_7kst.  Here I train the same model for 20 epochs.\n",
        "\n",
        "Some points to note regarding the `plot_forecasts` diagnostic plot:\n",
        " * on validation data not test data\n",
        " * `plot_forecasts`\n",
        "   * plot example forecasts with observations and lagged temperatures\n",
        "      * first row shows examples of best near zero rmse forecasts\n",
        "      * second row shows examples of worst positive rmse forecasts\n",
        "      * third row shows examples of worst negative rmse forecasts\n",
        "      * lagged observations are negative\n",
        "      * the day of the year the forecast begins in and the rmse value is displayed above each sub-plot\n",
        "\n",
        "### Updated VAR model\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "AsS2y-8JkMk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxkFmWFNjNcr"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tools.eval_measures import rmse, medianabs\n",
        "\n",
        "\n",
        "def plot_baseline_metrics(metrics, main_title):\n",
        "  fig, axs = plt.subplots(1, 2, figsize = (14, 7))\n",
        "  fig.suptitle(main_title)\n",
        "  axs = axs.ravel()  # APL ftw!\n",
        "\n",
        "  methods = metrics.method.unique()\n",
        "\n",
        "  for method in methods:\n",
        "    met_df = metrics.query('metric == \"rmse\" & method == \"%s\"' % method)\n",
        "    axs[0].plot(met_df.horizon, met_df.value, color='blue', label='Updated VAR')\n",
        "\n",
        "  ivar_rmse = np.array([0.39, 0.52, 0.64, 0.75, 0.86, 0.96, 1.06, 1.15, 1.23,\n",
        "                        1.31, 1.38, 1.45, 1.51, 1.57, 1.63, 1.68, 1.73, 1.77,\n",
        "                        1.81, 1.85, 1.89, 1.92, 1.96, 1.99, 2.02, 2.05, 2.08,\n",
        "                        2.1 , 2.13, 2.15, 2.18, 2.2 , 2.22, 2.24, 2.26, 2.28,\n",
        "                        2.3 , 2.31, 2.33, 2.35, 2.36, 2.38, 2.39, 2.4 , 2.42,\n",
        "                        2.43, 2.44, 2.45])\n",
        "  steps = [i for i in range(1, len(ivar_rmse)+1)]\n",
        "  axs[0].plot(steps, ivar_rmse, color='black', label='Initial VAR')\n",
        "\n",
        "  axs[0].set_xlabel(\"horizon - half hour steps\")\n",
        "  axs[0].set_ylabel(\"rmse\")\n",
        "  # axs[0].legend(methods)\n",
        "\n",
        "\n",
        "  for method in methods:\n",
        "    met_df = metrics.query('metric == \"mae\" & method == \"%s\"' % method)\n",
        "    axs[1].plot(met_df.horizon, met_df.value, color='blue', label='Updated VAR')\n",
        "\n",
        "  ivar_mae = np.array([0.39, 0.49, 0.57, 0.66, 0.74, 0.83, 0.91, 0.98, 1.05,\n",
        "                       1.12, 1.18, 1.24, 1.29, 1.34, 1.39, 1.43, 1.47, 1.5 ,\n",
        "                       1.53, 1.56, 1.59, 1.62, 1.64, 1.66, 1.68, 1.7 , 1.72,\n",
        "                       1.73, 1.75, 1.76, 1.77, 1.78, 1.8 , 1.81, 1.82, 1.83,\n",
        "                       1.83, 1.84, 1.85, 1.85, 1.86, 1.86, 1.87, 1.87, 1.88,\n",
        "                       1.88, 1.89, 1.89])\n",
        "  axs[1].plot(steps, ivar_mae, color='black', label='Initial VAR')\n",
        "\n",
        "  axs[1].set_xlabel(\"horizon - half hour steps\")\n",
        "  axs[1].set_ylabel(\"mae\")\n",
        "  # axs[1].legend(methods)\n",
        "\n",
        "  plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def update_metrics(metrics, test_data, method, get_metrics,\n",
        "                   model = None,\n",
        "                   met_cols = ['type', 'method', 'metric', 'horizon', 'value']):\n",
        "  metrics_h = []\n",
        "\n",
        "  if method in ['SES', 'HWES']:\n",
        "    horizons = [i for i in range(4, 49, 4)]\n",
        "    horizons.insert(0, 1)\n",
        "  else:\n",
        "    # horizons = [1, 48]\n",
        "    horizons = range(1, 49)\n",
        "\n",
        "  if method in ['VAR']:\n",
        "    variates = 'multivariate'\n",
        "  else:\n",
        "    variates = 'univariate'\n",
        "\n",
        "  print(\"h\\trmse\\tmae\")\n",
        "  for h in horizons:\n",
        "    if method in ['VAR']:\n",
        "      rmse_h, mae_h = get_metrics(test_data, h, method, model)\n",
        "    else:\n",
        "      rmse_h, mae_h = get_metrics(test_data, h, method)\n",
        "\n",
        "    metrics_h.append(dict(zip(met_cols, [variates, method, 'rmse', h, rmse_h])))\n",
        "    metrics_h.append(dict(zip(met_cols, [variates, method,  'mae', h,  mae_h])))\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  metrics_method = pd.DataFrame(metrics_h, columns = met_cols)\n",
        "  metrics = metrics.append(metrics_method)\n",
        "\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# rolling_cv with pre-trained model\n",
        "def var_rolling_cv(data, horizon, method, model):\n",
        "    lags = model.k_ar  # lag order\n",
        "    i = lags\n",
        "    h = horizon\n",
        "    rmse_roll, mae_roll = [], []\n",
        "    endo_vars = ['y', 'dew.point', 'humidity', 'pressure']\n",
        "    exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "                 'irradiance', 'azimuth_cos', 'za_rad'\n",
        "                ]\n",
        "\n",
        "    while (i + h) < len(data):\n",
        "        obs_df  = data[endo_vars].iloc[i:(i + h)]\n",
        "        endo_df = data[endo_vars].iloc[(i - lags):i].values\n",
        "        exog_df = data[exog_vars].iloc[i:(i + h)]\n",
        "\n",
        "        # y_hat = model.forecast(endo_df, steps = h)\n",
        "        y_hat = model.forecast(endo_df, exog_future = exog_df, steps = h)\n",
        "        preds = pd.DataFrame(y_hat, columns = endo_vars)\n",
        "\n",
        "        rmse_i = rmse(obs_df.y,      preds.y)\n",
        "        mae_i  = medianabs(obs_df.y, preds.y)\n",
        "        rmse_roll.append(rmse_i)\n",
        "        mae_roll.append(mae_i)\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "    print(h, '\\t', np.nanmean(rmse_roll).round(3), '\\t', np.nanmean(mae_roll).round(3))\n",
        "\n",
        "    return [np.nanmean(rmse_roll).round(2), np.nanmean(mae_roll).round(2)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DsHIe7adkVx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# approx. 5 mins\n",
        "\n",
        "train_df = train_df.asfreq(freq='30min')\n",
        "valid_df = valid_df.asfreq(freq='30min')\n",
        "test_df  = test_df.asfreq(freq='30min')\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "endo_vars = ['y', 'dew.point', 'humidity', 'pressure']\n",
        "exog_vars = [\n",
        "            'day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "            'irradiance', 'azimuth_cos', 'za_rad'\n",
        "            ]\n",
        "endo_df = train_df[endo_vars]\n",
        "exog_df = train_df[exog_vars]\n",
        "\n",
        "var_model = VAR(endog = endo_df, exog = exog_df)\n",
        "# var_model = VAR(endog = endo_df)\n",
        "MAX_LAGS = 96\n",
        "lag_order_res = var_model.select_order(MAX_LAGS)\n",
        "display(lag_order_res.summary())\n",
        "display(lag_order_res.selected_orders)\n",
        "print(lag_order_res.selected_orders['bic'])\n",
        "\n",
        "lag_order_table = lag_order_res.summary().data\n",
        "headers = lag_order_table.pop(0)\n",
        "lag_order_df = pd.DataFrame(lag_order_table, columns=headers)\n",
        "lag_order_df.drop('', axis=1, inplace=True)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    lag_order_df = pd.concat([lag_order_df[col].str.replace('*', '').astype(float)\n",
        "                             for col in lag_order_df], axis=1)\n",
        "\n",
        "lag_order_df.loc[1:, ['AIC','BIC','HQIC']].plot()\n",
        "plt.xlabel('lag')\n",
        "plt.ylabel('IC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mScNU7akW7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lowest BIC value occurs at 51 lags.  I'm going to use `maxlags = 51` because that is where decreasing returns sets in."
      ],
      "metadata": {
        "id": "eXY9mITnkk7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_var_backtest(model, data, endo_vars, exog_vars, y_col=Y_COL, horizon=HORIZON):\n",
        "  lags = model.k_ar  # lag order\n",
        "  i = lags\n",
        "  h = horizon\n",
        "  preds = []\n",
        "\n",
        "  while (i + h) < len(data):\n",
        "    if i % 1000 == 0:\n",
        "      print(i)\n",
        "\n",
        "    obs_df  = data[endo_vars].iloc[i:(i + h)]\n",
        "    endo_vals = data[endo_vars].iloc[(i - lags):i].values\n",
        "\n",
        "    if exog_vars is not None:\n",
        "      exog_df = data[exog_vars].iloc[i:(i + h)]\n",
        "      y_hat_lol = model.forecast(endo_vals, exog_future = exog_df, steps = h)\n",
        "    else:\n",
        "      y_hat_lol = model.forecast(endo_vals, steps = h)\n",
        "\n",
        "    y_col_pos = 0  # hardcoding is bad mkay - make function param?\n",
        "    y_hat_series = pd.Series(data  = [y_hat_l[y_col_pos] for y_hat_l in y_hat_lol],\n",
        "                             index = obs_df.index,\n",
        "                             name  = y_col)\n",
        "    y_hat_ts = TimeSeries.from_series(y_hat_series)\n",
        "    # y_hat_ts = TimeSeries.from_values(np.array([y_hat_l[y_col_pos] for y_hat_l in y_hat_lol]))\n",
        "    # y_hat = [y_hat_l[y_col_pos] for y_hat_l in y_hat_lol]\n",
        "\n",
        "    preds.append(y_hat_ts)\n",
        "    i = i + 1\n",
        "\n",
        "  return preds\n",
        "\n",
        "\n",
        "var_fit = var_model.fit(maxlags = 51, ic = 'bic')\n",
        "print(var_fit.summary())\n",
        "\n",
        "main_var_col = 'y'\n",
        "backtest_var = get_var_backtest(var_fit, valid_df, endo_vars, exog_vars, y_col = main_var_col)\n",
        "# display(len(backtest_var))\n",
        "# display(backtest_var[0])\n",
        "hist_comp_var = get_historic_comparison(backtest_var, valid_df, y_col = main_var_col)\n",
        "# display(hist_comp_var)\n",
        "summarise_historic_comparison(hist_comp_var, valid_df, y_col = main_var_col)\n",
        "\n",
        "title_var = 'VAR ' + main_var_col + '...'\n",
        "plot_multistep_diagnostics(hist_comp_var, title_var, y_col = main_var_col)\n",
        "\n",
        "\n",
        "# metric_cols = ['type', 'method', 'metric', 'horizon', 'value']\n",
        "# metrics = pd.DataFrame([], columns = metric_cols)\n",
        "# metrics = update_metrics(metrics, valid_df, 'VAR', var_rolling_cv, var_fit)\n",
        "## metrics = update_metrics(metrics, test_df, 'VAR', var_rolling_cv, var_fit)\n",
        "# plot_baseline_metrics(metrics, 'Multivariate Baseline Comparison - 2021 valid data')\n",
        "\n",
        "\n",
        "# 2019 data\n",
        "# maxlags = 5\n",
        "# ...\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.39 \t 0.39\n",
        "# 48 \t 2.45 \t 1.89\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# maxlags = 52\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.37 \t 0.37\n",
        "# 48 \t 2.253 \t 1.784\n",
        "# maxlags = 52 substantially better than maxlags = 9\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'humidity',]\n",
        "# maxlags = 52\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.37 \t 0.37\n",
        "# 48 \t 2.293 \t 1.814\n",
        "# including pressure is beneficial\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['za_rad', 'irradiance', 'azimuth_cos',]\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.369 \t 0.369\n",
        "# 48   2.163 \t 1.729\n",
        "# exog_vars is beneficial\n",
        "# 1 hr 28 mins :-(\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',]\n",
        "# maxlags = 52\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.37 \t 0.37\n",
        "# 48   2.133 \t 1.68\n",
        "# Sinusoidal terms better than irradiance etc!\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1', 'irradiance']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.369 \t 0.369\n",
        "# 48   2.105 \t 1.667\n",
        "# irradiance worth adding to sinusoidal terms\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1', 'za_rad']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.37 \t 0.37\n",
        "# 48   2.134 \t 1.679\n",
        "# za_rad not as beneficial as irradiance\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1', 'azimuth_cos']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.37 \t 0.37\n",
        "# 48   2.131 \t 1.675\n",
        "# azimuth_cos more beneficial than za_rad\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.368 \t 0.368\n",
        "# 48 \t 2.098 \t 1.658\n",
        "# Best model so far\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.368 \t 0.368\n",
        "# 48 \t 2.098 \t 1.657\n",
        "# Marginally better with za_rad\n",
        "\n",
        "# valid_df\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.347 \t 0.347\n",
        "# 48 \t 2.012 \t 1.581\n",
        "#\n",
        "\n",
        "# valid_df\n",
        "# endo_vars = ['y_des', 'dew.point_des', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 53\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.347 \t 0.347\n",
        "# 48 \t 2.724   2.132\n",
        "\n",
        "# valid_df\n",
        "# endo_vars = ['y_des', 'dew.point_des', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 53\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.357 \t 0.357\n",
        "# 48 \t 2.712   2.121\n",
        "\n",
        "# valid_df\n",
        "# train_df.loc['2016-01-01':,]\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 22\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.352 \t 0.352\n",
        "# 48 \t 2.926   2.305\n",
        "# Backtest RMSE 48th: 2.92592\n",
        "# Backtest MAE 48th:  2.304481\n",
        "# Radical decrease in maxlags!\n",
        "# Not a great model\n"
      ],
      "metadata": {
        "id": "JKNazIIokpEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rmse\n",
        "```\n",
        "[0.39, 0.52, 0.64, 0.75, 0.86, 0.96, 1.06, 1.15, 1.23,\n",
        " 1.31, 1.38, 1.45, 1.51, 1.57, 1.63, 1.68, 1.73, 1.77,\n",
        " 1.81, 1.85, 1.89, 1.92, 1.96, 1.99, 2.02, 2.05, 2.08,\n",
        " 2.1 , 2.13, 2.15, 2.18, 2.2 , 2.22, 2.24, 2.26, 2.28,\n",
        " 2.3 , 2.31, 2.33, 2.35, 2.36, 2.38, 2.39, 2.4 , 2.42,\n",
        " 2.43, 2.44, 2.45]\n",
        "```\n",
        "\n",
        "mae\n",
        "```\n",
        "[0.39, 0.49, 0.57, 0.66, 0.74, 0.83, 0.91, 0.98, 1.05,\n",
        " 1.12, 1.18, 1.24, 1.29, 1.34, 1.39, 1.43, 1.47, 1.5 ,\n",
        " 1.53, 1.56, 1.59, 1.62, 1.64, 1.66, 1.68, 1.7 , 1.72,\n",
        " 1.73, 1.75, 1.76, 1.77, 1.78, 1.8 , 1.81, 1.82, 1.83,\n",
        " 1.83, 1.84, 1.85, 1.85, 1.86, 1.86, 1.87, 1.87, 1.88,\n",
        " 1.88, 1.89, 1.89]\n",
        "```"
      ],
      "metadata": {
        "id": "4Q8X3C6Rk0s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_fit.plot()\n",
        "plt.show()\n",
        "\n",
        "# var_fit.plot_acorr()\n",
        "# plt.show()\n",
        "\n",
        "var_fit.fevd(48).plot()\n",
        "plt.show()\n",
        "\n",
        "var_fit.mse(48)"
      ],
      "metadata": {
        "id": "uNhTtXq6k6Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The updated VAR model shows substantial improvement.  It would benefit from further validation, including residual plots, QQ plots, autocorrelation of residual plots, residual boxplots across the forecast horizon steps etc\n",
        "\n",
        "NOTE: Updated VAR validated on 2021 data; initial VAR validated on 2019 data.\n",
        "\n",
        "TODO Move VAR baseline to separate notebook\n",
        "\n",
        "---\n",
        "\n",
        "**TODO** Plot model diagnostics.\n",
        "\n",
        "\n",
        "Next, I plot the best model and VAR model rmse and mae values for forecast horizons up to 48 (24 hours, each horizon step is equivalent to 30 minutes).  This plot plus the two others are for forecasts on the previously unused 2019 \"test\" data.  This is different from the 2018 \"validation\" data used elsewhere in this notebook.\n",
        "\n",
        "Some points to note regarding diagnostic plots:\n",
        " * once again, on test data not validation data\n",
        " * `plot_horizon_metrics`\n",
        "   * plot rmse and mae values for each individual step-ahead\n",
        " * `check_residuals`\n",
        "   * observations against predictions\n",
        "   * residuals over time\n",
        "   * residual distribution\n",
        " * `plot_forecasts`\n",
        "   * see sub-section immediately above for notable points"
      ],
      "metadata": {
        "id": "iOtK1fwHlJkw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrDh_OIHlL8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadly speaking, these results are very similar to the results from the VAR model.\n",
        "\n",
        "\n",
        "Diagnostic plots summary:\n",
        " * once again, these plots use test data not validation data\n",
        " * `plot_horizon_metrics`\n",
        "   * initially, these results look quite contradictory\n",
        "   * the rmse plot indicates better forecasts for the VAR method (in orange)\n",
        "   * the mae plot indicates better forecasts for the Conv2D kernel 2D method (in blue, mis-labelled as LSTM)\n",
        " * `check_residuals`\n",
        "   * the observations against predictions plot indicates\n",
        "     * predictions are too high at cold temperatures (below 0 C)\n",
        "     * predictions are too low at hot temperatures (above 25 C)\n",
        "   * residuals over time\n",
        "     * no obvious heteroscadicity\n",
        "     * no obvious periodicity\n",
        "       * surprising given observations against predictions plot\n",
        "   * residual distribution appears to be approximately normal (slightly right-skewed)\n",
        "     * no obvious sign of fat tails\n",
        " * `plot_forecasts`\n",
        "   * notable lack of noisy observations for the large positive and negative rmse examples\n",
        "\n",
        "The median absolute error (mae) is less sensitive to outliers compared to the root mean squared error (rmse) metric.\n",
        "\n",
        "Therefore, the rmse and mae plot difference may be due to the presence of outliers. I have maintained from the start that this data set is quite noisy, and attempts to correct these problems may have unintensionally introduced new issues.\n",
        "\n",
        "Transformed mean values across the 48 step horizon:\n",
        " * rmse of 2.05796\n",
        " * mae of 1.17986\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The best results from the gradient boosted trees are similar/different to  results from the [best LSTM model](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/lstm_time_series.ipynb).\n",
        "\n",
        "How and why are they similar/different?\n",
        "\n",
        "...\n",
        "\n",
        "The conclusion is separated into the following sections:\n",
        " 1. What worked\n",
        " 2. What didn't work\n",
        " 3. Rejected ideas\n",
        " 4. Future work\n"
      ],
      "metadata": {
        "id": "OvixNo_llMq-"
      }
    }
  ]
}