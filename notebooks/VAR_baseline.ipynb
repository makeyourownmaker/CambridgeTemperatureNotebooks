{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9IrAIaNk/7ZcgIczKrrQ0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VAR Baseline for Cambridge UK Weather Forecasting\n",
        "\n",
        "Gradient boosting models for time series analysis of Cambridge UK temperature measurements taken at the [University computer lab weather station](https://www.cl.cam.ac.uk/research/dtg/weather/).\n",
        "\n",
        "This notebook is being developed on [Google Colab](https://colab.research.google.com), using [LightGBM](https://lightgbm.readthedocs.io/) and the [Darts](https://unit8co.github.io/darts/) time series package.  Initially I was most interested in short term temperature forecasts (less than 2 hours) but now mostly produce results up to 24 hours in the future for comparison with earlier [baselines](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/cammet_baselines_2021.ipynb).\n",
        "\n",
        "See my previous notebooks, web apps etc:\n",
        " * [Cambridge UK temperature forecast python notebooks](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks)\n",
        " * [Cambridge UK temperature forecast R models](https://github.com/makeyourownmaker/CambridgeTemperatureModel)\n",
        " * [Bayesian optimisation of prophet temperature model](https://github.com/makeyourownmaker/BayesianProphet)\n",
        " * [Cambridge University Computer Laboratory weather station R shiny web app](https://github.com/makeyourownmaker/ComLabWeatherShiny)\n",
        "\n",
        "The linked notebooks, web apps etc contain further details including:\n",
        " * data description\n",
        " * data cleaning and preparation\n",
        " * data exploration\n",
        "\n",
        "In particular, see the notebooks:\n",
        " * [cammet_baselines_2021](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/cammet_baselines_2021.ipynb) including persistent, simple exponential smoothing, Holt Winter's exponential smoothing and vector autoregression\n",
        " * [keras_mlp_fcn_resnet_time_series](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/keras_mlp_fcn_resnet_time_series.ipynb), which uses a streamlined version of data preparation from [Tensorflow time series forecasting tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\n",
        " * [lstm_time_series](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/lstm_time_series.ipynb) with stacked LSTMs, bidirectional LSTMs and ConvLSTM1D networks\n",
        " * [cnn_time_series](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/cnn_time_series.ipynb) with Conv1D, multi-head Conv1D, Conv2D and Inception-style models\n",
        " * [encoder_decoder](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/encoder_decoder.ipynb) which includes autoencoder with attention, encoder decoder with teacher forcing, transformer with teacher forcing and padding, encoder only with MultiHeadAttention\n",
        " * [feature_engineering](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/feature_engineering.ipynb) solar-based and meteorology-based calculated features, rolling stats, tsfeatures, catch22, bivariate features and more\n",
        " * [tsfresh_feature_engineering](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/tsfresh_feature_engineering.ipynb) automated feature engineering and selection for time series analysis of Cambridge UK weather measurements\n",
        "\n",
        "Most of the above repositories, notebooks, web apps etc were built on both less data and less thoroughly cleaned data.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "\n",
        "**TODO** Add internal links before \"final\" commits\n",
        "\n",
        "Some sections may get added/deleted during development.\n",
        "\n",
        "Don't want any broken links, so finish later.\n",
        "\n",
        "\n",
        "Gradient Boosted Trees Introduction\n",
        "\n",
        "Code Setup\n",
        " * darts Installation\n",
        " * Library Imports\n",
        " * Environment Variables\n",
        " * Custom Functions\n",
        "\n",
        "Data Setup\n",
        " * Load pre-calculated features\n",
        " * See [feature_engineering.ipynb](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/feature_engineering.ipynb)\n",
        "\n",
        "LightGBM Models\n",
        " * Variable Selection\n",
        " * Lag Selection\n",
        " * Hyperparameter Tuning\n",
        "\n",
        "Comparison with Baselines\n",
        "\n",
        "Conclusion\n",
        " * What Worked\n",
        " * What Failed\n",
        " * Rejected Ideas\n",
        " * Future Work\n",
        "\n",
        "Metadata"
      ],
      "metadata": {
        "id": "QSQilOn5kG5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## VAR Baseline\n",
        "\n",
        "...\n",
        "\n",
        "### Load Libraries\n",
        "\n",
        "Load most of the required packages."
      ],
      "metadata": {
        "id": "fE1YsZgdl_KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import sys\n",
        "import math\n",
        "import timeit\n",
        "import datetime\n",
        "import itertools\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import acf, pacf, lagmat, coint\n",
        "from statsmodels.tsa.stattools import adfuller, kpss, grangercausalitytests\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "\n",
        "# Reduces variance in results but won't eliminate it :-(\n",
        "%env PYTHONHASHSEED=0\n",
        "import random\n",
        "\n",
        "# set seed to make all processes deterministic\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Prevent lightgbm 'Converting column-vector to 1d array' warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    action   = 'ignore',\n",
        "    category = UserWarning,\n",
        "    module   = r'.*lightgbm'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UZReUS3bmR-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Custom Functions\n",
        "\n",
        "Next, define some utility functions:\n",
        " * `rmse_`\n",
        " * `mse_`\n",
        " * `mae_`\n",
        " * `summarise_backtest`\n",
        " * `print_rmse_mae`\n",
        " * `drop_cols_correlated_with_feat_cols`\n",
        " * `drop_problem_cols`\n",
        " * `summarise_historic_comparison`\n",
        " * `plot_lagged_feat_imp_subplot`\n",
        " * `get_pastcov_features`\n",
        " * `get_pastcov_lags`\n",
        " * `plot_lagged_feature_importances`\n",
        " * `plot_feature_importances`\n",
        " * `get_feature_importances`\n",
        " * `expand_grid`\n",
        " * `keep_key`\n",
        " * `get_historic_comparison`\n",
        " * `_plot_xy_for_label`\n",
        " * `plot_multistep_obs_vs_preds`\n",
        " * `plot_multistep_obs_vs_mean_preds_by_step`\n",
        " * `plot_multistep_obs_preds_dists`\n",
        " * `plot_multistep_residuals`\n",
        " * `plot_multistep_residuals_dist`\n",
        " * `plot_multistep_residuals_vs_predicted`\n",
        " * `se_`\n",
        " * `metric_ci_vals`\n",
        " * `plot_horizon_metrics`\n",
        " * `plot_horizon_metrics_boxplots`\n",
        " * `plot_multistep_diagnostics`\n",
        " * `_filter_out_missing`\n",
        " * `plot_multistep_forecast_examples`\n",
        " * `get_rmse_mae_from_backtest`\n",
        " * `plot_catboost_learning_curve`\n",
        " * `plot_lgb_learning_curve`\n",
        " * `drop_correlated_cols`\n",
        " * `get_feature_selection_scores`\n",
        " * `plot_observation_examples`\n",
        " * `sanity_check_df_rows_cols_labels`\n",
        " * `sanity_check_before_after_dfs`\n",
        " * `sanity_check_train_valid_test`\n",
        " * `print_train_valid_test_shapes`\n",
        " * `plot_feature_history`\n",
        " * `plot_feature_history_separately`\n",
        " * `check_high_low_thresholds`\n",
        " * `get_features_filename`\n",
        " * `merge_data_and_aggs`\n",
        " * `get_rolling_features`\n",
        " * `finalise_rolling_features`\n",
        " * `print_null_columns`\n",
        " * `print_na_locations`\n",
        " * `get_features`\n",
        " * `get_darts_series`\n",
        " * `plot_short_term_acf`\n",
        " * `plot_long_term_acf`\n"
      ],
      "metadata": {
        "id": "V0sZuz-dmwbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _check_obs_preds_lens_eq(obs, preds):\n",
        "    obs_preds_lens_eq = 1\n",
        "\n",
        "    if len(obs) != len(preds):\n",
        "        print(\"obs:  \", len(obs))\n",
        "        print(\"preds:\", len(preds))\n",
        "        obs_preds_lens_eq = 0\n",
        "\n",
        "    return obs_preds_lens_eq\n",
        "\n",
        "\n",
        "def rmse_(obs, preds):\n",
        "    if _check_obs_preds_lens_eq(obs, preds) == 0:\n",
        "        stop()\n",
        "    else:\n",
        "        return np.sqrt(np.mean((obs - preds) ** 2))\n",
        "\n",
        "\n",
        "def mse_(obs, preds):\n",
        "    if _check_obs_preds_lens_eq(obs, preds) == 0:\n",
        "        stop()\n",
        "    else:\n",
        "        return np.mean((obs - preds) ** 2)\n",
        "\n",
        "\n",
        "def mae_(obs, preds):\n",
        "    \"mean absolute error - equivalent to the keras loss function\"\n",
        "    if _check_obs_preds_lens_eq(obs, preds) == 0:\n",
        "        stop()\n",
        "    else:\n",
        "        return np.mean(np.abs(obs - preds))      # keras loss\n",
        "        # return np.median(np.abs(obs - preds))  # earlier baselines\n",
        "\n",
        "\n",
        "# TODO Remove me?\n",
        "def summarise_backtest(backtest, df, horizon = HORIZON, digits = 6, y_col = Y_COL):\n",
        "\n",
        "    if len(backtest[0]) == 1:\n",
        "        print(\"\\n# Backtest RMSE:\", round(rmse_(val_ser[-len(backtest):].values(), backtest.values()), digits))\n",
        "        print(\"# Backtest MAE: \",   round( mae_(val_ser[-len(backtest):].values(), backtest.values()), digits))\n",
        "\n",
        "        print(\"\\nbacktest[\", y_col, \"]:\\n\", sep='')\n",
        "        backtest_stats = stats.describe(backtest[y_col].values())\n",
        "        print(\"count\\t\", backtest_stats[0])\n",
        "        print(\"mean\\t\",  round(backtest_stats[2][0], digits))\n",
        "        print(\"std\\t\",   round(np.sqrt(backtest_stats[3][0]), digits))\n",
        "        print(\"min\\t\",   round(np.min(backtest_stats[1]), digits))\n",
        "        print(\"25%\\t\",   round(np.percentile(backtest[y_col].values(), 25), digits))\n",
        "        print(\"50%\\t\",   round(np.median(backtest[y_col].values()), digits))\n",
        "        print(\"75%\\t\",   round(np.percentile(backtest[y_col].values(), 75), digits))\n",
        "        print(\"max\\t\",   round(np.max(backtest_stats[1]), digits))\n",
        "    elif len(backtest[0]) == horizon:\n",
        "        preds_df = pd.concat([backtest[i].pd_dataframe() for i in range(len(backtest))], axis=0)\n",
        "        trues_df = df.loc[preds_df.index, [y_col]]\n",
        "        hist_comp = pd.concat([trues_df, preds_df[y_col]], axis = 1)\n",
        "        hist_comp.columns = [y_col, 'pred']\n",
        "        list_int = [i for i in range(1, horizon + 1)]\n",
        "        reps = len(hist_comp) // len(list_int)\n",
        "        hist_comp['step'] = np.tile(list_int, reps)\n",
        "\n",
        "        print(\"\\nBacktest RMSE all:\", round(rmse_(hist_comp[y_col], hist_comp['pred']), digits))\n",
        "        print(\"Backtest MAE all: \",    round(mae_(hist_comp[y_col], hist_comp['pred']), digits))\n",
        "\n",
        "        print(\"\\n# Backtest RMSE 48th:\", round(rmse_(hist_comp.loc[hist_comp['step'] == horizon, y_col], \\\n",
        "                                                     hist_comp.loc[hist_comp['step'] == horizon, 'pred']), digits))\n",
        "        print(\"# Backtest MAE 48th: \",    round(mae_(hist_comp.loc[hist_comp['step'] == horizon, y_col], \\\n",
        "                                                     hist_comp.loc[hist_comp['step'] == horizon, 'pred']), digits))\n",
        "\n",
        "        lasttest_stats = stats.describe(hist_comp['pred'])\n",
        "        print(\"\\nbacktest[\", y_col, \"]:\\n\", sep='')\n",
        "        print(\"count\\t\", len(hist_comp['pred']))\n",
        "        print(\"mean\\t\",  round(lasttest_stats[2], digits))\n",
        "        print(\"std\\t\",   round(np.sqrt(lasttest_stats[3]), digits))\n",
        "        print(\"min\\t\",   round(np.min(lasttest_stats[1]), digits))\n",
        "        print(\"25%\\t\",   round(np.percentile(hist_comp['pred'], 25), digits))\n",
        "        print(\"50%\\t\",   round(np.median(hist_comp['pred']), digits))\n",
        "        print(\"75%\\t\",   round(np.percentile(hist_comp['pred'], 75), digits))\n",
        "        print(\"max\\t\",   round(np.max(lasttest_stats[1]), digits))\n",
        "\n",
        "\n",
        "def print_rmse_mae(obs, preds, postfix_str, prefix_str = '', digits = 6):\n",
        "    print(prefix_str, \"Backtest RMSE \", postfix_str, \": \",\n",
        "          round(rmse_(obs, preds), digits),\n",
        "          sep='')\n",
        "    print(prefix_str, \"Backtest MAE \",  postfix_str, \":  \",\n",
        "          round( mae_(obs, preds), digits),\n",
        "          sep='')\n",
        "    print()\n",
        "\n",
        "\n",
        "def drop_cols_correlated_with_feat_cols(df, feats_df, threshold=0.95):\n",
        "\n",
        "  for feat_col in feats_df.columns:\n",
        "    corrs = df.corrwith(feats_df[feat_col])\n",
        "    drop_cols = corrs[(corrs > threshold) & (corrs != 1.0)]\n",
        "\n",
        "    for i in range(len(drop_cols)):\n",
        "      drop_col = drop_cols.index[i]\n",
        "      if drop_col in df.columns:  # and drop_col not in feats_df.columns:\n",
        "        del df[drop_col]\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def drop_problem_cols(df, lag, drop_cor=True,\n",
        "                      var_cutoff=0.05, cor_cutoff=0.95, na_cutoff=0.05,\n",
        "                      verbose = False):\n",
        "\n",
        "  if verbose:\n",
        "    print('drop_problem_cols - start:', df.shape)\n",
        "\n",
        "\n",
        "  # drop all NA columns\n",
        "  df = df.dropna(axis = 1, how = 'all')\n",
        "\n",
        "  if verbose:\n",
        "    print('drop_problem_cols - after dropna:', df.shape)\n",
        "\n",
        "\n",
        "  # drop single value columns\n",
        "  df = df.loc[:, (df != df.iloc[lag]).any()]\n",
        "\n",
        "  if verbose:\n",
        "    print('drop_problem_cols - after drop single value cols:', df.shape)\n",
        "\n",
        "\n",
        "  # drop low variance columns\n",
        "  if 'ds' in df.columns:\n",
        "    df = df.drop(['ds'], axis=1)\n",
        "    df = df.loc[:, df.std() > var_cutoff]\n",
        "    df['ds'] = df.index\n",
        "  else:\n",
        "    df = df.loc[:, df.std() > var_cutoff]\n",
        "\n",
        "  if verbose:\n",
        "    print('drop_problem_cols - after drop low var cols:', df.shape)\n",
        "\n",
        "\n",
        "  # drop highly correlated columns\n",
        "  if drop_cor:\n",
        "    df = drop_correlated_cols(df, cor_cutoff)\n",
        "\n",
        "  if verbose:\n",
        "    print('drop_problem_cols - after drop correlated cols:', df.shape)\n",
        "\n",
        "\n",
        "  # drop cols with high % of NA values\n",
        "  pc_thresh = int(na_cutoff * df.shape[0])\n",
        "  #print('five_pc_thresh:', five_pc_thresh)\n",
        "  print('columns with null values:')\n",
        "  display(df.isnull().sum())\n",
        "  df = df.loc[:, df.isnull().sum() < pc_thresh]\n",
        "\n",
        "  if verbose:\n",
        "    print('drop_problem_cols - after drop high % of NAs:', df.shape)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def summarise_historic_comparison(hc, df, horizon = HORIZON,\n",
        "                                  digits = 6,\n",
        "                                  y_col = Y_COL,\n",
        "                                  df_name = 'valid_df'):\n",
        "\n",
        "    print('\\n')\n",
        "    print_rmse_mae(hc[y_col], hc['pred'], 'all')\n",
        "\n",
        "    obs   = hc.loc[hc['step'] == horizon, y_col]\n",
        "    preds = hc.loc[hc['step'] == horizon, 'pred']\n",
        "    if horizon == 1:\n",
        "      post_str = '1st'\n",
        "    elif horizon == 2:\n",
        "      post_str = '2nd'\n",
        "    elif horizon == 3:\n",
        "      post_str = '3rd'\n",
        "    else:\n",
        "      post_str = str(horizon) + 'th'\n",
        "    print_rmse_mae(obs, preds, post_str, '# ')\n",
        "\n",
        "    obs   = hc.loc[hc['missing'] == 0.0, y_col]\n",
        "    preds = hc.loc[hc['missing'] == 0.0, 'pred']\n",
        "    print_rmse_mae(obs, preds, 'miss==0')\n",
        "\n",
        "    obs   = hc.loc[hc['missing'] == 1.0, y_col]\n",
        "    preds = hc.loc[hc['missing'] == 1.0, 'pred']\n",
        "    print_rmse_mae(obs, preds, 'miss==1')\n",
        "\n",
        "    if y_col == 'y_des':\n",
        "      # preds = hc['pred'] - hc['y_seasonal']\n",
        "      preds = hc['pred'] - hc['y_yearly'] - hc['y_daily'] - hc['y_trend']\n",
        "    elif y_col == 'y_des_fft':\n",
        "      preds = hc['pred'] - hc['y_fft']\n",
        "    elif y_col == 'y':\n",
        "      preds = hc['pred']\n",
        "    elif y_col == 'y_res':\n",
        "      preds = hc['pred'] - hc['y_yearly'] - hc['y_daily']\n",
        "\n",
        "    preds.dropna(inplace=True)\n",
        "    lasttest_stats = stats.describe(preds)\n",
        "    print(\"\\nbacktest['\", y_col, \"']:\", sep='')\n",
        "    print(\"count\\t\", len(preds))\n",
        "    print(\"mean\\t\",  round(lasttest_stats[2], digits))\n",
        "    print(\"std\\t\",   round(np.sqrt(lasttest_stats[3]), digits))\n",
        "    print(\"min\\t\",   round(np.min(lasttest_stats[1]), digits))\n",
        "    print(\"25%\\t\",   round(np.percentile(preds, 25), digits))\n",
        "    print(\"50%\\t\",   round(np.median(preds), digits))\n",
        "    print(\"75%\\t\",   round(np.percentile(preds, 75), digits))\n",
        "    print(\"max\\t\",   round(np.max(lasttest_stats[1]), digits))\n",
        "\n",
        "    print(\"\\n\", df_name, \"['\", y_col, \"']:\\n\", df[y_col].describe(), '\\n', sep='')\n",
        "\n",
        "\n",
        "def plot_lagged_feat_imp_subplot(fi_df, subset):\n",
        "    bar_height = 0.25\n",
        "    title = 'Feature importance'\n",
        "\n",
        "    fi_max = fi_df['importance'].max()\n",
        "    if fi_max > 100:\n",
        "      xl_max = int(np.ceil(fi_max / 100.0)) * 100\n",
        "    elif fi_max > 10:\n",
        "      xl_max = int(np.ceil(fi_max / 10.0)) * 10\n",
        "    else:\n",
        "      xl_max = fi_max\n",
        "\n",
        "    if subset is not None:\n",
        "      fi_df = fi_df.loc[fi_df['feature'].str.contains(subset, regex=True), :]\n",
        "      title += ' - ' + subset + ' features'\n",
        "      plt.figure(figsize=(7, 3))\n",
        "    else:\n",
        "      title += ' - all features'\n",
        "      plt.figure(figsize=(20, 10))\n",
        "\n",
        "    plt.xlim(0, xl_max)\n",
        "    plt.barh(width  = fi_df['importance'],\n",
        "             y      = fi_df['feature'],\n",
        "             height = bar_height)\n",
        "\n",
        "\n",
        "    def plot_highlighted_lagged_feat_imp_subset(data, subset_str, hl_col, bar_height = 0.25):\n",
        "        data_subset = data.loc[data['feature'].str.contains(subset_str, regex=True), :]\n",
        "\n",
        "        plt.barh(width  = data_subset['importance'],\n",
        "                 y      = data_subset['feature'],\n",
        "                 height = bar_height,\n",
        "                 color  = hl_col)\n",
        "\n",
        "\n",
        "    plot_highlighted_lagged_feat_imp_subset(fi_df, 'shadow', 'red')\n",
        "\n",
        "    yregex = '^' + Y_COL\n",
        "    ytarg_str = yregex + '_target_'\n",
        "    plot_highlighted_lagged_feat_imp_subset(fi_df, ytarg_str, 'green')\n",
        "\n",
        "    ypcov_str = yregex + '_pastcov_'\n",
        "    plot_highlighted_lagged_feat_imp_subset(fi_df, ypcov_str, 'blue')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_pastcov_features(fi_df):\n",
        "    pcov_feats_long = fi_df.loc[fi_df['feature'].str.contains('_pastcov_'), 'feature'].to_list()\n",
        "\n",
        "    r = re.compile('_pastcov_.*$')\n",
        "\n",
        "    pcov_feats_dups = [r.sub('', pcov_feat_long) for pcov_feat_long in pcov_feats_long]\n",
        "    pcov_feats = list(set(pcov_feats_dups))\n",
        "\n",
        "    if Y_COL in pcov_feats:\n",
        "      pcov_feats.remove(Y_COL)\n",
        "\n",
        "    return pcov_feats\n",
        "\n",
        "\n",
        "def get_pastcov_lags(fi_df):\n",
        "    pcov_feats_long = fi_df.loc[fi_df['feature'].str.contains('_pastcov_'), 'feature'].to_list()\n",
        "\n",
        "    r = re.compile('^.*_pastcov_')\n",
        "\n",
        "    pcov_lags_dups = [r.sub('', pcov_feat_long) for pcov_feat_long in pcov_feats_long]\n",
        "    pcov_lags = list(set(pcov_lags_dups))\n",
        "\n",
        "    return pcov_lags\n",
        "\n",
        "\n",
        "# TODO: Also, consider combining plot_feature_importances and\n",
        "#       plot_lagged_feature_importances into a single function\n",
        "def plot_lagged_feature_importances(model):\n",
        "    '''Plot feature importance for models with multiple lags\n",
        "\n",
        "    Should be easier to compare importance across features, lags, targets\n",
        "    and past covariates\n",
        "\n",
        "    No support for future covariates\n",
        "\n",
        "    Use plot_feature_importances function for lags = 1, past_cov_lags = 1 models\n",
        "\n",
        "    '''\n",
        "\n",
        "    imp_thresh = 0\n",
        "    if isinstance(model, CatBoostModel):\n",
        "      imp_thresh = 0  # some catboost importance values below 1\n",
        "    elif isinstance(model, LightGBMModel):\n",
        "      imp_thresh = 1\n",
        "\n",
        "    imp_df = pd.DataFrame({'feature':    model.lagged_feature_names,\n",
        "                           'importance': model.model.feature_importances_})\n",
        "    imp_df = imp_df.sort_values('importance')\n",
        "\n",
        "    plot_lagged_feat_imp_subplot(imp_df, None)\n",
        "    plot_lagged_feat_imp_subplot(imp_df, '^' + Y_COL)\n",
        "\n",
        "    pcov_feats = get_pastcov_features(imp_df)\n",
        "    for pcov_feat in pcov_feats:\n",
        "        plot_lagged_feat_imp_subplot(imp_df, '^' + pcov_feat)\n",
        "\n",
        "    pcov_lags = get_pastcov_lags(imp_df)\n",
        "    for pcov_lag in pcov_lags:\n",
        "        plot_lagged_feat_imp_subplot(imp_df, '_pastcov_' + pcov_lag + '$')\n",
        "\n",
        "\n",
        "# TODO: Consider combining plot_feature_importances and\n",
        "#       plot_lagged_feature_importances into a single function\n",
        "def plot_feature_importances(model, \\\n",
        "                             y_col        = Y_COL, \\\n",
        "                             include_cols = None,  \\\n",
        "                             exclude_cols = None):\n",
        "    '''Plot feature importances from lightGBM models\n",
        "\n",
        "    WARNING: Only works with lags = 1 and lags_past_cov = 1\n",
        "             Use plot_lagged_feature_importances for models with additional lags\n",
        "    '''\n",
        "\n",
        "    imp_thresh = 0\n",
        "    if isinstance(model, CatBoostModel):\n",
        "      imp_thresh = 0  # some catboost importance values below 1\n",
        "    elif isinstance(model, LightGBMModel):\n",
        "      imp_thresh = 1\n",
        "\n",
        "    if include_cols is not None:\n",
        "        col_names = include_cols\n",
        "    else:\n",
        "        col_names = model.lagged_feature_names\n",
        "\n",
        "    cols_df = pd.DataFrame({'feature': col_names,\n",
        "                            'importance': model.model.feature_importances_})\n",
        "    cols_df = cols_df.sort_values('importance')\n",
        "    cols_df = cols_df[cols_df.importance >= imp_thresh]\n",
        "\n",
        "    if exclude_cols is not None:\n",
        "      cols_df = cols_df[~cols_df['feature'].isin(exclude_cols)]\n",
        "\n",
        "    # print(\"cols_df:\", cols_df, sep='\\n')\n",
        "\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.barh(width  = cols_df['importance'],\n",
        "             y      = cols_df['feature'],\n",
        "             height = 0.25);\n",
        "    plt.barh(width  = cols_df.loc[cols_df['feature'].str.contains('shadow'),\n",
        "                                  'importance'],\n",
        "             y      = cols_df.loc[cols_df['feature'].str.contains('shadow'),\n",
        "                                  'feature'],\n",
        "             height = 0.25,\n",
        "             color  = 'red')\n",
        "    plt.title('Feature importance\\nimportance threshold = ' + str(imp_thresh))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_feature_importances(model,\n",
        "                            y_col = Y_COL,\n",
        "                            imp_thresh   = None,\n",
        "                            include_cols = None,\n",
        "                            exclude_cols = None,\n",
        "                            verbose      = True):\n",
        "\n",
        "    # imp_thresh = 0\n",
        "    if imp_thresh is None and isinstance(model, CatBoostModel):\n",
        "      imp_thresh = 0  # some catboost importance values below 1\n",
        "    elif imp_thresh is None and isinstance(model, LightGBMModel):\n",
        "      imp_thresh = 1\n",
        "\n",
        "    if include_cols is not None:\n",
        "        # col_names = pd.Series(include_cols)\n",
        "        col_names = include_cols\n",
        "    else:\n",
        "        col_names = model.lagged_feature_names\n",
        "\n",
        "    cols_df = pd.DataFrame({'feature': col_names,\n",
        "                            'importance': model.model.feature_importances_})\n",
        "    cols_df = cols_df.sort_values('importance')\n",
        "    cols_df = cols_df[cols_df.importance >= imp_thresh]\n",
        "\n",
        "    if exclude_cols is not None:\n",
        "      cols_df = cols_df[~cols_df['feature'].isin(exclude_cols)]\n",
        "\n",
        "    # print(\"cols_df:\", cols_df, sep='\\n')\n",
        "\n",
        "    shadow_str = '_shadow_'\n",
        "    if shadow_str in ''.join(cols_df['feature'].values):\n",
        "        shad_thresh = cols_df.loc[cols_df['feature'].str.contains(shadow_str),\n",
        "                                  'importance'].tail(1).values[0]\n",
        "    else:\n",
        "        shad_thresh = 0.0\n",
        "    # print('shad_thresh:', shad_thresh)\n",
        "\n",
        "    cols_df = cols_df[cols_df['importance'] > shad_thresh]\n",
        "\n",
        "    if verbose:\n",
        "      print(cols_df.to_string(index=False), sep='\\n')\n",
        "\n",
        "    inc_cols = []\n",
        "    for feature in cols_df['feature'].values:\n",
        "        inc_cols.append(re.sub('_(pastcov|target|futcov)_lag.*', '', feature))\n",
        "\n",
        "    # remove duplicates from a list, while preserving order\n",
        "    seen = set()\n",
        "    inc_cols = [col for col in inc_cols if col not in seen and not seen.add(col)]\n",
        "\n",
        "    if verbose:\n",
        "      print('\\ninc_cols:', inc_cols)\n",
        "\n",
        "    return inc_cols\n",
        "\n",
        "\n",
        "def expand_grid(dictionary):\n",
        "   return pd.DataFrame([row for row in product(*dictionary.values())],\n",
        "                       columns = dictionary.keys())\n",
        "\n",
        "\n",
        "def keep_key(d, k):\n",
        "  \"\"\" models = keep_key(models, 'datasets') \"\"\"\n",
        "  return {k: d[k]}\n",
        "\n",
        "\n",
        "def get_historic_comparison(backtest, df, y_col = Y_COL, horizon = HORIZON):\n",
        "    if horizon > 1:\n",
        "      assert len(backtest[0]) > 1\n",
        "\n",
        "    if y_col == 'y_des':\n",
        "      # cols = ['y_des', 'y_seasonal']\n",
        "      cols = ['y_des', 'y_yearly', 'y_daily', 'y_trend']\n",
        "    elif y_col == 'y_des_fft':\n",
        "      cols = ['y_des_fft', 'y_fft']\n",
        "    elif y_col == 'y_res':\n",
        "      cols = ['y_res', 'y_yearly', 'y_daily']\n",
        "    elif y_col == 'y':\n",
        "      cols = ['y']\n",
        "\n",
        "    # cols.extend(['missing', 'mi_filled', 'isd_outlier', 'hist_average'])\n",
        "    cols.extend(['missing', 'isd_outlier'])\n",
        "\n",
        "    preds_df = pd.concat([backtest[i].pd_dataframe() for i in range(len(backtest))], axis=0)\n",
        "    trues_df = df.loc[preds_df.index, cols]\n",
        "\n",
        "    hist_comp = pd.concat([trues_df, preds_df[y_col]], axis = 1)\n",
        "    cols.append('pred')\n",
        "    hist_comp.columns = cols\n",
        "\n",
        "    # re-seasonalise\n",
        "    if y_col == 'y_des':\n",
        "      hist_comp['y_des'] += hist_comp['y_yearly'] + hist_comp['y_daily'] + hist_comp['y_trend']\n",
        "      hist_comp['pred']  += hist_comp['y_yearly'] + hist_comp['y_daily'] + hist_comp['y_trend']\n",
        "    elif y_col == 'y_des_fft':\n",
        "      hist_comp['y_des_fft'] += hist_comp['y_fft']\n",
        "      hist_comp['pred']      += hist_comp['y_fft']\n",
        "    elif y_col == 'y_res':\n",
        "      hist_comp['y_res'] += hist_comp['y_yearly'] + hist_comp['y_daily']\n",
        "      hist_comp['pred']  += hist_comp['y_yearly'] + hist_comp['y_daily']\n",
        "\n",
        "\n",
        "    hist_comp['res']    = hist_comp[y_col] - hist_comp['pred']\n",
        "    hist_comp['res^2']  = hist_comp['res'] * hist_comp['res']\n",
        "    hist_comp['res_sign']  = np.sign(hist_comp['res'])\n",
        "    hist_comp['missing']   = hist_comp['missing']#.astype(int)\n",
        "    # hist_comp['mi_filled'] = hist_comp['mi_filled']#.astype(int)\n",
        "    # hist_comp['hist_average'] = hist_comp['hist_average']#.astype(int)\n",
        "\n",
        "    list_int = [i for i in range(1, horizon+1)]\n",
        "    reps = len(hist_comp) // len(list_int)\n",
        "    hist_comp['step'] = np.tile(list_int, reps)\n",
        "    hist_comp['id']   = np.repeat([i for i in range(reps)], horizon)\n",
        "    hist_comp['date'] = hist_comp.index.values\n",
        "\n",
        "    return hist_comp\n",
        "\n",
        "\n",
        "def plot_one_step_abs_err_boxplot(one_step, title):\n",
        "  one_step['abs_err'] = np.abs(one_step['res'])\n",
        "  one_step[['abs_err']].boxplot(meanline  = False,\n",
        "                                showmeans = True,\n",
        "                                showcaps  = True,\n",
        "                                showbox   = True,\n",
        "                                # showfliers = False,\n",
        "                                )\n",
        "  plt.title(title + '\\nboxplot with mean and median')\n",
        "  plt.suptitle('')\n",
        "  plt.ylabel('absolute error')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_step_residuals_dist(one_step, title):\n",
        "  plt.figure(figsize = (12, 16))\n",
        "  plt.subplot(5, 1, 5)\n",
        "  pd.Series(one_step['res']).plot(kind = 'density', label='residuals')\n",
        "  plt.xlim(-10, 10)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_step_residuals(one_step, title):\n",
        "  x_miss = one_step.loc[one_step['missing'] == 1.0, 'obs'].index\n",
        "  y_miss = one_step.loc[one_step['missing'] == 1.0, 'res']\n",
        "\n",
        "  plt.figure(figsize = (12, 16))\n",
        "  plt.subplot(5, 1, 4)\n",
        "  plt.scatter(x = one_step.index, y = one_step['res'])\n",
        "  plt.scatter(x_miss, y_miss, color='red', label='missing')\n",
        "  plt.axhline(y = 0, color = 'grey')\n",
        "  plt.xlabel('Index position')\n",
        "  plt.ylabel('Residuals')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_step_obs_preds_dists(one_step, title):\n",
        "  obs   = one_step['obs']\n",
        "  preds = one_step['preds']\n",
        "  r2score = r2_score(obs, preds)\n",
        "\n",
        "  plt.figure(figsize = (12, 16))\n",
        "  plt.subplot(5, 1, 3)\n",
        "  pd.Series(obs).plot(kind = 'density', label='observations')\n",
        "  pd.Series(preds).plot(kind = 'density', label='predictions')\n",
        "  plt.xlim(-10, 40)\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "  plt.annotate(\"$R^2$ = {:.3f}\".format(r2score), (-7.5, 0.055))\n",
        "  # plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_step_obs_vs_preds(one_step, title):\n",
        "\n",
        "  obs   = one_step['obs']\n",
        "  preds = one_step['preds']\n",
        "  x_miss = one_step.loc[one_step['missing'] == 1.0, 'obs']\n",
        "  y_miss = one_step.loc[one_step['missing'] == 1.0, 'preds']\n",
        "\n",
        "  r2score = r2_score(obs, preds)\n",
        "\n",
        "  plt.figure(figsize = (12, 16))\n",
        "  plt.subplot(5, 1, 1)\n",
        "  plt.scatter(x = obs, y = preds)\n",
        "  plt.scatter(x_miss, y_miss, color='red', label='missing')\n",
        "  plt.axline((0, 0), slope=1.0, color=\"grey\")\n",
        "  plt.xlabel('Observations')\n",
        "  plt.ylabel('Predictions')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.annotate(\"$R^2$ = {:.3f}\".format(r2score), (-9, 31))\n",
        "  plt.title(title)\n",
        "  plt.xlim((-10, 35))\n",
        "  plt.ylim((-10, 35))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_step_diagnostics(model, data, val_series, val_pastcov_series, title, val_fut_cov=None):\n",
        "  plot_feature_importances(model)\n",
        "\n",
        "  # re-seasonalise observations\n",
        "  if Y_COL == 'y_des':\n",
        "    obs = data['y_des'] + data['y_yearly'] + data['y_daily'] + data['y_trend']\n",
        "  elif Y_COL == 'y_des_fft':\n",
        "    obs = data['y_des_fft'] + data['y_fft']\n",
        "  else:\n",
        "    obs = data[Y_COL]\n",
        "\n",
        "  # print('data:', data.shape)\n",
        "  # display(data[['y_des_fft', 'y_fft']])\n",
        "  # print('obs:', obs.shape)\n",
        "  # display(obs)\n",
        "\n",
        "  if val_fut_cov is None:\n",
        "    res = model.residuals(series = val_series,\n",
        "                          past_covariates = val_pastcov_series,\n",
        "                          retrain = False).pd_series()\n",
        "  else:\n",
        "    res = model.residuals(series = val_series,\n",
        "                          past_covariates = val_pastcov_series,\n",
        "                          future_covariates = val_fut_cov,\n",
        "                          retrain = False).pd_series()\n",
        "\n",
        "  preds = obs + res\n",
        "  preds = preds.dropna()\n",
        "  obs  = obs[preds.index]\n",
        "  res  = res[preds.index]\n",
        "  miss = data.loc[preds.index, 'missing']\n",
        "\n",
        "  print_rmse_mae(obs, preds, '1st', '# ')\n",
        "\n",
        "  one_step = pd.concat([obs, preds, res, miss], axis=1)\n",
        "  one_step.columns = ['obs', 'preds', 'res', 'missing']\n",
        "\n",
        "  title = 'step = 1 ' + title\n",
        "  plot_one_step_obs_vs_preds(one_step, title)\n",
        "  # plot_obs_vs_mean_preds_by_step(hist, title)\n",
        "  plot_one_step_obs_preds_dists(one_step, title)\n",
        "  plot_one_step_residuals(one_step, title + ' residuals')\n",
        "  plot_one_step_residuals_dist(one_step, title + ' residuals density')\n",
        "  plot_one_step_residuals_acf(one_step, title + ' residuals acf')\n",
        "  plot_one_step_residuals_qq(one_step, title + ' residuals qq-plot')\n",
        "  plot_one_step_abs_err_boxplot(one_step, title)\n",
        "\n",
        "\n",
        "def plot_one_step_residuals_qq(one_step, title_):\n",
        "  fig, axs = plt.subplots(figsize=(6, 6))\n",
        "  sm.qqplot(one_step['res'], line='q', ax=axs)\n",
        "  axs.set_title(title_)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_one_step_residuals_acf(one_step, title_, max_lags = 300):\n",
        "  plt.figure(figsize = (6, 6))\n",
        "\n",
        "  acf = pd.DataFrame()\n",
        "  acf_feat = 'res'\n",
        "\n",
        "  acf[acf_feat] = [one_step[acf_feat].autocorr(l) for l in range(1, max_lags)]\n",
        "  plt.plot(acf[acf_feat], label='residual')\n",
        "\n",
        "  plt.axhline(0, linestyle='--', c='black')\n",
        "  plt.ylabel('autocorrelation')\n",
        "  plt.xlabel('time lags')\n",
        "  plt.title(title_)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def _plot_xy_for_label(data, label, x_feat, y_feat, color):\n",
        "    x = data.loc[data[label] == 1.0, x_feat]\n",
        "    y = data.loc[data[label] == 1.0, y_feat]\n",
        "\n",
        "    if len(x) > 0:\n",
        "        plt.scatter(x = x, y = y, color=color, alpha=0.5, label=label)\n",
        "\n",
        "\n",
        "def plot_multistep_obs_vs_preds(hist, title, y_col=Y_COL):\n",
        "    plt.figure(figsize = (12, 16))\n",
        "    plt.subplot(5, 1, 1)\n",
        "    plt.scatter(x = hist[y_col], y = hist['pred'])\n",
        "    _plot_xy_for_label(hist, 'missing',      y_col, 'pred', 'red')\n",
        "    # _plot_xy_for_label(hist, 'hist_average', y_col, 'pred', 'yellow')\n",
        "    # _plot_xy_for_label(hist, 'mi_filled',    y_col, 'pred', 'purple')\n",
        "    plt.axline((0, 0), slope=1.0, color=\"grey\")\n",
        "    plt.xlabel('Observations')\n",
        "    plt.ylabel('Predictions')\n",
        "    plt.legend(loc='lower right')\n",
        "    obs   = hist.loc[hist[[y_col, 'pred']].notnull().all(1), y_col]\n",
        "    preds = hist.loc[hist[[y_col, 'pred']].notnull().all(1), 'pred']\n",
        "    r2score = r2_score(obs, preds)\n",
        "    plt.annotate(\"$R^2$ = {:.3f}\".format(r2score), (-9, 31))\n",
        "    plt.title(title)\n",
        "    plt.xlim((-10, 35))\n",
        "    plt.ylim((-10, 35))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_multistep_obs_vs_mean_preds_by_step(hist, title, y_col = Y_COL,\n",
        "                                             step_ = HORIZON, ci = False):\n",
        "    '''For specific step, plot mean prediction for each observation\n",
        "\n",
        "    A 95 % confidence interval is plotted, but can be disabled\n",
        "    '''\n",
        "\n",
        "    mean_preds = hist.loc[hist['step'] == step_, [y_col, 'pred']].groupby(y_col).mean('pred')\n",
        "    obs   = mean_preds.index.values\n",
        "    preds = mean_preds['pred'].values\n",
        "\n",
        "    plt.figure(figsize = (12, 16))\n",
        "    ax = plt.subplot(5, 1, 2)\n",
        "    plt.plot(obs, preds)\n",
        "\n",
        "    if ci is True:\n",
        "      ci = 1.96 * np.std(preds) / np.sqrt(len(obs))\n",
        "      # print(ci)\n",
        "      ax.fill_between(obs, (preds - ci), (preds + ci), color='b', alpha=.1)\n",
        "\n",
        "    plt.axline((0, 0), slope=1.0, color=\"grey\")\n",
        "    r2score = r2_score(obs, preds)\n",
        "    plt.annotate(\"$R^2$ = {:.3f} - step = {}\".format(r2score, step_), (-9, 31))\n",
        "    plt.title(title + ' step = ' + str(step_))\n",
        "    plt.xlabel('Temperature')\n",
        "    plt.ylabel('Mean prediction')\n",
        "    plt.xlim((-10, 35))\n",
        "    plt.ylim((-10, 35))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_multistep_obs_preds_dists(hist, title, y_col=Y_COL):\n",
        "    obs   = hist.loc[hist[[y_col, 'pred']].notnull().all(1), y_col]\n",
        "    preds = hist.loc[hist[[y_col, 'pred']].notnull().all(1), 'pred']\n",
        "    r2score = r2_score(obs, preds)\n",
        "    plt.figure(figsize = (12, 16))\n",
        "    plt.subplot(5, 1, 3)\n",
        "    pd.Series(obs).plot(kind = 'density', label='observations')\n",
        "    pd.Series(preds).plot(kind = 'density', label='predictions')\n",
        "    plt.xlim(-10, 40)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.annotate(\"$R^2$ = {:.3f}\".format(r2score), (-7.5, 0.055))\n",
        "    #plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_multistep_residuals(hist, title):\n",
        "    plt.figure(figsize = (12, 16))\n",
        "    plt.subplot(5, 1, 4)\n",
        "    plt.scatter(x = range(len(hist)), y = hist['res'])\n",
        "    hist['id.2'] = range(len(hist))\n",
        "    _plot_xy_for_label(hist, 'missing',      'id.2', 'res', 'red')\n",
        "    # _plot_xy_for_label(hist, 'hist_average', 'id.2', 'res', 'yellow')\n",
        "    # _plot_xy_for_label(hist, 'mi_filled',    'id.2', 'res', 'purple')\n",
        "    plt.axhline(y = 0, color = 'grey')\n",
        "    plt.xlabel('Index position')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_multistep_residuals_dist(hist, title):\n",
        "    plt.figure(figsize = (12, 16))\n",
        "    plt.subplot(5, 1, 5)\n",
        "    pd.Series(hist['res']).plot(kind = 'density', label='residuals')\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Unused?\n",
        "# TODO Diagonal structure of these plots might need further consideration\n",
        "#      Add lowess fit to check for problems\n",
        "def plot_multistep_residuals_vs_predicted(hist, title):\n",
        "    plt.subplot(5, 1, 5)\n",
        "    plt.scatter(x = hist['pred'], y = hist['res'])\n",
        "    _plot_xy_for_label(hist, 'missing',      'pred', 'res', 'red')\n",
        "    # _plot_xy_for_label(hist, 'hist_average', 'pred', 'res', 'yellow')\n",
        "    # _plot_xy_for_label(hist, 'mi_filled',    'pred', 'res', 'purple')\n",
        "    plt.axhline(y = 0, color = 'grey')\n",
        "\n",
        "    n = 24  # slow to run all points :-(\n",
        "            # 12 takes approx 2 mins to run\n",
        "            #  8 takes approx 4 mins to run\n",
        "    xy = hist.iloc[::n, :]\n",
        "    # x = hist.iloc[::n, :]\n",
        "    y_l = lowess(xy['res'], xy['pred'])\n",
        "    plt.plot(y_l[:, 0], y_l[:, 1], 'green', label='lowess fit')\n",
        "\n",
        "    plt.xlabel('Predictions')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(title);\n",
        "\n",
        "\n",
        "def se_(obs, preds, metric):\n",
        "    '''Standard error of sum of squared residuals or sum of absolute residuals'''\n",
        "\n",
        "    if _check_obs_preds_lens_eq(obs, preds) == 0:\n",
        "        stop()\n",
        "\n",
        "    if metric == 'rmse':\n",
        "        se = np.sqrt(np.sum((obs - preds) ** 2) / len(obs))\n",
        "    elif metric == 'mae':\n",
        "        se = np.sqrt(np.sum(np.abs(obs - preds)) / len(obs))\n",
        "    else:\n",
        "        print('Unrecognised metric:', metric)\n",
        "        print(\"metric should be 'rmse' or 'mae'\")\n",
        "        stop()\n",
        "\n",
        "    return se\n",
        "\n",
        "\n",
        "def metric_ci_vals(test_val, se, z_val = 1.95996):\n",
        "    cil = z_val * se\n",
        "    # print('cil:', cil)\n",
        "    metric_cil = test_val - cil\n",
        "    metric_ciu = test_val + cil\n",
        "\n",
        "    return metric_cil, metric_ciu\n",
        "\n",
        "\n",
        "# TODO: Remove unused confidence intervals\n",
        "# NOTE: VAR baseline metrics cvar_rmse and cvar_mae hardcoded to 48 steps\n",
        "def plot_horizon_metrics(hist, title, y_col=Y_COL, horizon = HORIZON, ci=False):\n",
        "    steps = [i for i in range(1, horizon+1)]\n",
        "\n",
        "    # calculate metrics\n",
        "    z_val_95 = 1.95996\n",
        "    z_val_50 = 0.674\n",
        "    rmse_h,   mae_h    = np.zeros(horizon), np.zeros(horizon)\n",
        "    res_se_h, abs_se_h = np.zeros(horizon), np.zeros(horizon)\n",
        "    rmse_ciu, rmse_cil = np.zeros(horizon), np.zeros(horizon)\n",
        "    mae_ciu,  mae_cil  = np.zeros(horizon), np.zeros(horizon)\n",
        "\n",
        "    for i in range(1, horizon+1):\n",
        "      obs   = hist.loc[hist['step'] == i, y_col]\n",
        "      preds = hist.loc[hist['step'] == i, 'pred']\n",
        "      rmse_h[i-1] = rmse_(obs, preds)\n",
        "      mae_h[i-1]  =  mae_(obs, preds)\n",
        "      res_se_h[i-1] = se_(obs, preds, 'rmse')\n",
        "      abs_se_h[i-1] = se_(obs, preds, 'mae')\n",
        "      # mae_h[i]  = np.median(np.abs(obs - preds))  # for comparison with baselines\n",
        "      rmse_cil[i-1], rmse_ciu[i-1] = metric_ci_vals(rmse_h[i-1], res_se_h[i-1], z_val_50)\n",
        "      mae_cil[i-1],  mae_ciu[i-1]  = metric_ci_vals(mae_h[i-1],  abs_se_h[i-1], z_val_50)\n",
        "\n",
        "    # print('rmse_h:', rmse_h)\n",
        "    # print('mae_h:',  mae_h)\n",
        "\n",
        "    # plot metrics for horizons\n",
        "    fig, axs = plt.subplots(1, 2, figsize = (14, 7))\n",
        "    fig.suptitle(title + ' forecast horizon errors')\n",
        "    axs = axs.ravel()\n",
        "\n",
        "\n",
        "    mean_val_lab = title + ' mean value'\n",
        "    axs[0].plot(steps, rmse_h, color='green', label=title)\n",
        "\n",
        "    if ci is True:\n",
        "      axs[0].fill_between(steps, rmse_cil, rmse_ciu, color='green', alpha=0.25)\n",
        "\n",
        "    # i - initial, u - updated, c - corrected\n",
        "    #ivar_rmse = np.array([0.39, 0.52, 0.64, 0.75, 0.86, 0.96, 1.06, 1.15, 1.23,\n",
        "    #                     1.31, 1.38, 1.45, 1.51, 1.57, 1.63, 1.68, 1.73, 1.77,\n",
        "    #                     1.81, 1.85, 1.89, 1.92, 1.96, 1.99, 2.02, 2.05, 2.08,\n",
        "    #                     2.1 , 2.13, 2.15, 2.18, 2.2 , 2.22, 2.24, 2.26, 2.28,\n",
        "    #                     2.3 , 2.31, 2.33, 2.35, 2.36, 2.38, 2.39, 2.4 , 2.42,\n",
        "    #                     2.43, 2.44, 2.45])\n",
        "    # NOTE: uvar_rmse tested on test_df\n",
        "    #uvar_rmse = np.array([0.36, 0.49, 0.6, 0.7, 0.8, 0.89, 0.98, 1.06, 1.14,\n",
        "    #                      1.21, 1.28, 1.35, 1.41, 1.47, 1.52, 1.57, 1.62, 1.66,\n",
        "    #                      1.7, 1.74, 1.78, 1.81, 1.84, 1.87, 1.9, 1.93, 1.96,\n",
        "    #                      1.99, 2.01, 2.03, 2.06, 2.08, 2.1, 2.12, 2.14, 2.16,\n",
        "    #                      2.18,  2.19, 2.21, 2.23, 2.24, 2.26, 2.27, 2.29, 2.3,\n",
        "    #                      2.31, 2.33, 2.34])\n",
        "    cvar_rmse = np.array([0.49318888, 0.70222546, 0.88570688, 1.05495349,\n",
        "    1.21081157, 1.34945832, 1.46844034, 1.57779714, 1.67754323, 1.7665827,\n",
        "    1.84567039, 1.91561743, 1.97899766, 2.03616174, 2.08661944, 2.13396441,\n",
        "    2.17809725, 2.21946156, 2.25780078, 2.29370568, 2.3272055,  2.35760153,\n",
        "    2.38520845, 2.41076185, 2.43404716, 2.45466806, 2.47361784, 2.49117761,\n",
        "    2.50625606, 2.52023589, 2.53319205, 2.54566125, 2.55764924, 2.56870554,\n",
        "    2.57976955, 2.59102429, 2.6018822, 2.61242356, 2.62280045, 2.63353767,\n",
        "    2.64410312, 2.65458709, 2.66532837, 2.67609086, 2.68675178, 2.69745108,\n",
        "    2.71002892, 2.72445726])\n",
        "    #axs[0].plot(steps, ivar_rmse, color='black', label='Initial VAR')\n",
        "    axs[0].plot(steps, cvar_rmse, color='blue', label='Updated VAR')\n",
        "    axs[0].hlines(np.mean(rmse_h), xmin=1, xmax=horizon,\n",
        "                  color='green', linestyles='dotted', label=mean_val_lab)\n",
        "    axs[0].hlines(np.mean(cvar_rmse), xmin=1, xmax=horizon,\n",
        "                  color='blue', linestyles='dotted', label='Updated VAR mean value')\n",
        "    axs[0].set_xlabel(\"horizon - half hour steps\")\n",
        "    axs[0].set_ylabel(\"rmse\")\n",
        "\n",
        "\n",
        "    axs[1].plot(steps, mae_h, color='green', label=title)\n",
        "\n",
        "    if ci is True:\n",
        "      axs[1].fill_between(steps, mae_cil, mae_ciu, color='green', alpha=0.25)\n",
        "\n",
        "    # NOTE: ivar_mae tested on test_df\n",
        "    #ivar_mae = np.array([0.39, 0.49, 0.57, 0.66, 0.74, 0.83, 0.91, 0.98, 1.05,\n",
        "    #                    1.12, 1.18, 1.24, 1.29, 1.34, 1.39, 1.43, 1.47, 1.5 ,\n",
        "    #                    1.53, 1.56, 1.59, 1.62, 1.64, 1.66, 1.68, 1.7 , 1.72,\n",
        "    #                    1.73, 1.75, 1.76, 1.77, 1.78, 1.8 , 1.81, 1.82, 1.83,\n",
        "    #                    1.83, 1.84, 1.85, 1.85, 1.86, 1.86, 1.87, 1.87, 1.88,\n",
        "    #                    1.88, 1.89, 1.89])\n",
        "    #uvar_mae = np.array([0.36, 0.45, 0.53, 0.61, 0.69, 0.76, 0.83, 0.9, 0.97,\n",
        "    #                     1.03, 1.09, 1.14, 1.19, 1.24, 1.28, 1.32, 1.36, 1.4,\n",
        "    #                     1.43, 1.46, 1.49, 1.52, 1.54, 1.56, 1.58, 1.6, 1.62,\n",
        "    #                     1.63, 1.65, 1.66, 1.68, 1.69, 1.7, 1.71, 1.72, 1.73,\n",
        "    #                     1.74, 1.74, 1.75, 1.75, 1.76, 1.76, 1.77, 1.77, 1.78,\n",
        "    #                     1.78, 1.78, 1.78])\n",
        "    cvar_mae = np.array([0.34694645, 0.50765333, 0.65132003, 0.78584432,\n",
        "    0.9077075,  1.01705088, 1.11113622, 1.19759807, 1.27696634, 1.34941444,\n",
        "    1.4134705,  1.47180058, 1.52304802, 1.56961154, 1.60903759, 1.64763418,\n",
        "    1.68391297, 1.71690735, 1.74787094, 1.77721642, 1.80442554, 1.82951782,\n",
        "    1.85358226, 1.87488643, 1.89346337, 1.91069565, 1.92613218, 1.94071845,\n",
        "    1.95245349, 1.96323923, 1.9736734,  1.98370815, 1.99367508, 2.00204077,\n",
        "    2.00992601, 2.01796976, 2.02747736, 2.03477489, 2.04173317, 2.04985428,\n",
        "    2.05843847, 2.06731348, 2.07606609, 2.08533656, 2.09560914, 2.10668272,\n",
        "    2.1183637,  2.13164371])\n",
        "    #axs[1].plot(steps, ivar_mae, color='black', label='Initial VAR')\n",
        "    axs[1].plot(steps, cvar_mae, color='blue', label='Updated VAR')\n",
        "    axs[1].hlines(np.mean(mae_h), xmin=1, xmax=horizon,\n",
        "                  color='green', linestyles='dotted', label=mean_val_lab)\n",
        "    axs[1].hlines(np.mean(cvar_mae), xmin=1, xmax=horizon,\n",
        "                  color='blue', linestyles='dotted', label='Updated VAR mean value')\n",
        "    axs[1].set_xlabel(\"horizon - half hour steps\")\n",
        "    axs[1].set_ylabel(\"mae\")\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_horizon_metrics_boxplots(hist, title):\n",
        "\n",
        "  hist['abs_err'] = np.abs(hist['res'])\n",
        "  hist[['abs_err', 'step']].boxplot(by='step',\n",
        "                                    meanline=False,\n",
        "                                    showmeans=True,\n",
        "                                    showcaps=True,\n",
        "                                    showbox=True,\n",
        "                                    showfliers=False,\n",
        "                                    )\n",
        "  plt.title(title + '\\nboxplots with mean and median')\n",
        "  plt.suptitle('')\n",
        "  plt.xlabel(\"horizon - half hour steps\")\n",
        "  plt.ylabel(\"absolute error\")\n",
        "  x_step = 10.0\n",
        "  x_max  = np.ceil(np.max(hist.step) / x_step) * int(x_step)\n",
        "  plt.xticks(np.arange(0, x_max, int(x_step)))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_multistep_diagnostics(hist, title, y_col=Y_COL):\n",
        "  title = 'Multi-step ' + title\n",
        "  plot_multistep_obs_vs_preds(hist, title, y_col)\n",
        "  plot_multistep_obs_vs_mean_preds_by_step(hist, title, y_col)\n",
        "  plot_multistep_obs_preds_dists(hist, title, y_col)\n",
        "  plot_multistep_residuals(hist, title + ' residuals')\n",
        "  plot_multistep_residuals_dist(hist, title + ' residuals density')\n",
        "  plot_horizon_metrics(hist, title, y_col)\n",
        "  plot_horizon_metrics_boxplots(hist, title)\n",
        "  # plot_multistep_forecast_examples(hist, title + ' forecast examples')\n",
        "\n",
        "\n",
        "# TODO Refactor this\n",
        "#      miss, preds, obs, res, err, dates etc \"family\" of variables\n",
        "#      is a warning sign\n",
        "#      try-catch around lagged_miss is clear indication of upsteam issues\n",
        "#      Consider using a better data structure\n",
        "#      See also: plot_forecast_examples immediately below\n",
        "def _filter_out_missing(pos_neg_rmse_all, miss, lags, subplots):\n",
        "    '''Check if obs (lags and horizon) missing == 1.0\n",
        "    and\n",
        "    Avoid contiguous indices'''\n",
        "\n",
        "    # print(\"pos_neg_rmse_all:\", pos_neg_rmse_all)\n",
        "\n",
        "    pos_neg_rmse = pd.Series(subplots)\n",
        "    subplot_count = j = 0\n",
        "\n",
        "    while subplot_count < subplots:\n",
        "      restart = False\n",
        "      idx = pos_neg_rmse_all.index[j]\n",
        "      # print(j, idx, pos_neg_rmse_all.loc[pos_neg_rmse_all.index[j]])\n",
        "\n",
        "      # Avoid indices in the first few observations\n",
        "      # Would be incomplete\n",
        "      if idx < lags:\n",
        "        # print('idx < lags:', idx)\n",
        "        j += 1\n",
        "        continue\n",
        "\n",
        "      # Avoid contiguous indices - don't want 877, 878, 879\n",
        "      if subplot_count > 0:\n",
        "        for i in range(subplot_count):\n",
        "          if abs(idx - pos_neg_rmse[i]) < lags:\n",
        "            # print('contiguous indices - idx, pos_neg_rmse[i]:', idx, pos_neg_rmse[i])\n",
        "            restart = True\n",
        "            break\n",
        "\n",
        "      if restart is False:\n",
        "        try:\n",
        "            lagged_miss = (miss.loc[idx - lags, :] == 1.0).any()\n",
        "        except KeyError:\n",
        "            lagged_miss = True\n",
        "\n",
        "        horizon_miss = (miss.loc[idx, :] == 1.0).any()\n",
        "        missing = lagged_miss or horizon_miss\n",
        "        # print(\"\\nlagged_miss:\", lagged_miss)\n",
        "        # print(\"horizon_miss:\",  horizon_miss)\n",
        "        # print(\"missing:\", missing)\n",
        "\n",
        "        if missing is False:\n",
        "          pos_neg_rmse[subplot_count] = idx\n",
        "          subplot_count += 1\n",
        "        #else:\n",
        "        #  print('missing')\n",
        "\n",
        "      j += 1\n",
        "\n",
        "    return pos_neg_rmse\n",
        "\n",
        "\n",
        "# TODO Refactor this\n",
        "#      miss, preds, obs, res, err, dates etc \"family\" of variables\n",
        "#      is a warning sign\n",
        "#      Consider using a better data structure\n",
        "#      See also: _filter_out_missing immediately above\n",
        "def plot_multistep_forecast_examples(hist, title, subplots = 3, horizon = HORIZON, lags = 48):\n",
        "    \"\"\"Plot example forecasts with observations and lagged temperatures.\n",
        "       Ensure examples are non-contiguous.\n",
        "\n",
        "       First row shows near zero rmse forecasts.\n",
        "       Second row shows most positive rmse forecasts.\n",
        "       Third row shows most negative rmse forecasts.\n",
        "\n",
        "       missing == 0 - ie no imputation for missing data\n",
        "    \"\"\"\n",
        "\n",
        "    assert subplots in [3, 4, 5]\n",
        "\n",
        "    # hist = hist.dropna()\n",
        "\n",
        "    col = 'step'\n",
        "    id_col = 'id'\n",
        "    miss  = hist.pivot_table(index=id_col, columns=col, values='missing')\n",
        "    preds = hist.pivot_table(index=id_col, columns=col, values='pred')\n",
        "    obs   = hist.pivot_table(index=id_col, columns=col, values='y_des')\n",
        "    res   = hist.pivot_table(index=id_col, columns=col, values='res')\n",
        "    err   = hist.pivot_table(index=id_col, columns=col, values='res^2')\n",
        "    dates = hist.pivot_table(index=id_col, columns=col, values='date')\n",
        "\n",
        "    miss.dropna(inplace=True)\n",
        "    # print(\"miss:\", miss.shape)\n",
        "    preds.dropna(inplace=True)\n",
        "    # print(\"preds:\", preds.shape)\n",
        "    # obs.dropna(inplace=True)\n",
        "    # print(\"obs:\", obs.shape)\n",
        "    res.dropna(inplace=True)\n",
        "    # print(\"res:\", res.shape)\n",
        "    err.dropna(inplace=True)\n",
        "    # print(\"err:\", err.shape)\n",
        "    dates.dropna(inplace=True)\n",
        "    # print(\"dates:\", dates.shape)\n",
        "    dates = dates.iloc[err.index, :]\n",
        "    # print(\"dates indexed:\", dates.shape)\n",
        "\n",
        "    # res_sign = np.sign(-res.mean(axis = 1))\n",
        "    # err_row_means = err.mean(axis = 1)\n",
        "    # rmse_rows = res_sign * np.sqrt(err_row_means)\n",
        "    err_row_means = np.sum(err, axis = 1) / horizon\n",
        "    res_sum = np.sum(res, axis = 1)\n",
        "    # print(\"res_sum:\",  len(res_sum))\n",
        "    # print(res_sum[0:5])\n",
        "    res_sign  = np.sign(np.sum(res, axis = 1))\n",
        "    rmse_rows = res_sign * np.sqrt(err_row_means)\n",
        "    # print(\"rmse_rows:\", len(rmse_rows))\n",
        "    # print(\"res_sign:\",  len(res_sign))\n",
        "    # print(res_sign[0:5])\n",
        "\n",
        "    # choose forecasts - check for missing == 0\n",
        "    # neg_rmse_all = np.argsort(rmse_rows)\n",
        "    ##pos_rmse_all = np.flip(np.argsort(rmse_rows))\n",
        "    # pos_rmse_all = np.argsort(-rmse_rows)\n",
        "    neg_rmse_all = rmse_rows.sort_values()\n",
        "    # print(rmse_rows.loc[neg_rmse_all.index])\n",
        "    pos_rmse_all = neg_rmse_all[::-1]\n",
        "    # print(rmse_rows.loc[pos_rmse_all.index])\n",
        "    nz_rmse_all  = rmse_rows.abs().sort_values()\n",
        "    # print(rmse_rows.loc[nz_rmse_all.index])\n",
        "    # nz_rmse_all  = np.argsort(np.abs(rmse_rows))  # nz near zero\n",
        "    # print(\"\\nneg_rmse_all:\", len(neg_rmse_all))\n",
        "    # print(rmse_rows[neg_rmse_all[0:5]])\n",
        "    # print(\"pos_rmse_all:\", len(pos_rmse_all))\n",
        "    # print(rmse_rows[pos_rmse_all[0:5]])\n",
        "    # print(\"nz_rmse_all: \", len(nz_rmse_all))\n",
        "    # print(rmse_rows[nz_rmse_all[0:5]])\n",
        "\n",
        "    nz_rmse  = _filter_out_missing(nz_rmse_all,  miss, lags, subplots)\n",
        "    pos_rmse = _filter_out_missing(pos_rmse_all, miss, lags, subplots)\n",
        "    neg_rmse = _filter_out_missing(neg_rmse_all, miss, lags, subplots)\n",
        "\n",
        "    plot_idx = np.concatenate((nz_rmse, pos_rmse, neg_rmse))\n",
        "    # print(\"\\nplot_idx:\", len(plot_idx))\n",
        "    # print(\"\\nplot_idx:\", plot_idx)\n",
        "\n",
        "    # plot forecasts\n",
        "    fig, axs = plt.subplots(3, subplots, sharey = True, figsize = (15, 10))\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace = 0.3, top = 0.87)\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    myFmt = mdates.DateFormatter('%H:%M')\n",
        "\n",
        "    for i in range(3 * subplots):\n",
        "      # print(\"plot_idx[i] - lags:\",plot_idx[i],  plot_idx[i] - lags)\n",
        "      axs[i].plot(dates.iloc[plot_idx[i] - lags, :],\n",
        "                  obs.loc[plot_idx[i] - lags, :],\n",
        "                  'blue',\n",
        "                  label='lagged observations')\n",
        "\n",
        "      axs[i].plot(dates.iloc[plot_idx[i], :],\n",
        "                  obs.loc[plot_idx[i], :],\n",
        "                  'green',\n",
        "                  label='observations')\n",
        "\n",
        "      axs[i].plot(dates.iloc[plot_idx[i], :],\n",
        "                  preds.loc[plot_idx[i], :],\n",
        "                  'orange',\n",
        "                  label='forecast')\n",
        "\n",
        "      axs[i].xaxis.set_major_formatter(myFmt)\n",
        "      obs_dates = dates.iloc[plot_idx[i] - lags, :]\n",
        "      sub_title = \"{0}, {1:d}, {2:.3f}\".format(obs_dates.iloc[0],\n",
        "                                               plot_idx[i],\n",
        "                                               rmse_rows.loc[plot_idx[i]])\n",
        "      axs[i].title.set_text(sub_title)\n",
        "\n",
        "    fig.suptitle(title + \"\\ninit date, period idx, signed rmse\")\n",
        "    fig.text(0.5, 0.04, 'hour', ha='center')\n",
        "    fig.text(0.04, 0.5, 'Temperature - $^\\circ$C', va='center', rotation='vertical')\n",
        "    plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "    plt.show();\n",
        "\n",
        "\n",
        "# WARN This function probably has too many arguments - consider refactoring\n",
        "def get_rmse_mae_from_backtest(model, param_df, i, series, past_cov, data, prefix, horizon=HORIZON, digits=6):\n",
        "    backtest = model.historical_forecasts(series = series,\n",
        "                                          past_covariates = past_cov,\n",
        "                                          start   = 0.01,\n",
        "                                          retrain = False,\n",
        "                                          verbose = True,\n",
        "                                          forecast_horizon = horizon,\n",
        "                                          last_points_only = False)\n",
        "    hc = get_historic_comparison(backtest, data)\n",
        "    obs   = hc.loc[hc['step'] == horizon, Y_COL]\n",
        "    preds = hc.loc[hc['step'] == horizon, 'pred']\n",
        "    param_df.at[i, prefix + '_rmse'] = round(rmse_(obs, preds), digits)\n",
        "    param_df.at[i, prefix + '_mae']  = round(mae_(obs,  preds), digits)\n",
        "\n",
        "    return param_df\n",
        "\n",
        "\n",
        "def plot_lgb_learning_curve(models, title = None, metric = 'l2', margin = None):\n",
        "    '''Plot learning curve for lightgbm models using the lightgbm plot_metric function\n",
        "\n",
        "    evals_results_ for validation data missing in action\n",
        "    So, build 2 models - first with training data for validation\n",
        "                       - second with validation data for validation\n",
        "                       - pass both models in as a list\n",
        "                       - order of models is important\n",
        "\n",
        "    Training and validation curves are plotted when model.fit is called with\n",
        "    both training and validation data:\n",
        "    model.fit(series,\n",
        "              past_covariates = past_cov,\n",
        "              val_series = val_ser,\n",
        "              val_past_covariates = val_past_cov)\n",
        "\n",
        "    Primarily tested with catboost\n",
        "    '''\n",
        "\n",
        "    assert len(models) == 2\n",
        "\n",
        "    final_rmse = []\n",
        "\n",
        "    for model in models:\n",
        "      assert hasattr(model, 'model')\n",
        "      assert hasattr(model.model, 'evals_result_')\n",
        "\n",
        "      final_rmse.append(model.model.evals_result_['valid_0'][metric][-1])\n",
        "\n",
        "    if margin is None:\n",
        "      lgb.plot_metric(models[0].model.evals_result_)\n",
        "    else:\n",
        "      assert margin > 0.0\n",
        "      y_lim_min = min(final_rmse) - margin\n",
        "      y_lim_max = max(final_rmse) + margin\n",
        "\n",
        "      if y_lim_min < 0.0:\n",
        "        y_lim_min = 0.0\n",
        "\n",
        "      y_lim = (y_lim_min, y_lim_max)\n",
        "\n",
        "      lgb.plot_metric(models[0].model.evals_result_, ylim = y_lim)\n",
        "\n",
        "    plt.plot(models[1].model.evals_result_['valid_0']['l2'])\n",
        "    plt.gca().get_lines()[0].set_color('blue')\n",
        "\n",
        "    labels_ = ['train']\n",
        "    if len(plt.gca().get_lines()) == 1 and plt.gca().get_label() == 'valid_0':\n",
        "      labels_ = ['valid']\n",
        "\n",
        "    if len(plt.gca().get_lines()) > 1:\n",
        "      plt.gca().get_lines()[1].set_color('orange')\n",
        "      labels_.append('valid')\n",
        "\n",
        "    if title is not None:\n",
        "      plt.title(title)\n",
        "\n",
        "    plt.legend(labels = labels_)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_learning_curves(models, title, margin=0.05):\n",
        "  print(\"\\n\")\n",
        "\n",
        "  if type(models) is list and len(models) == 2:\n",
        "    plot_lgb_learning_curve(models, title)\n",
        "    plot_lgb_learning_curve(models, title, margin = margin)\n",
        "  else:\n",
        "    print('Unsupported number of models: ', len(models))\n",
        "    print('models should have length 1 or 2!')\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_main_plot_title(pre_str, lag_params, mod_params):\n",
        "  lag_params_str = ', '.join([f\"{' '.join(map(str, v))}\" for v in lag_params.items()])\n",
        "  mod_params_str = ', '.join([f\"{' '.join(map(str, v))}\" for v in mod_params.items()])\n",
        "  plot_title = pre_str + lag_params_str + '\\n' + mod_params_str\n",
        "\n",
        "  return plot_title\n",
        "\n",
        "\n",
        "def build_two_lgbm_models(mod_params, data_params, train, valid):\n",
        "  '''lgbm only for now ...'''\n",
        "\n",
        "  series, past_cov, fut_cov = get_darts_series(train.loc['2016-01-12':,], data_params)\n",
        "  val_ser, val_past_cov, val_fut_cov = get_darts_series(valid, data_params)\n",
        "\n",
        "  # add_encoders3 = {'cyclic': {'future': ['minute', 'hour', 'dayofyear']}}\n",
        "  #                           #'past':   ['minute', 'hour', 'dayofyear']}}\n",
        "  # model_tr1 = LightGBMModel(**mod_params, add_encoders=add_encoders3)\n",
        "  # model1    = LightGBMModel(**mod_params, add_encoders=add_encoders3)\n",
        "  model_tr1 = LightGBMModel(**mod_params)\n",
        "  model1    = LightGBMModel(**mod_params)\n",
        "\n",
        "  if data_params['fut_cov_cols'] is not None:\n",
        "    model_tr1.fit(series,\n",
        "                  past_covariates = past_cov,\n",
        "                  future_covariates = fut_cov,\n",
        "                  val_series = series,\n",
        "                  val_past_covariates = past_cov,\n",
        "                  val_future_covariates = fut_cov,\n",
        "                  callbacks = [lgb.log_evaluation(0)]\n",
        "                 )\n",
        "  else:\n",
        "    model_tr1.fit(series,\n",
        "                  past_covariates = past_cov,\n",
        "                  val_series = series,\n",
        "                  val_past_covariates = past_cov,\n",
        "                  callbacks = [lgb.log_evaluation(0)]\n",
        "                 )\n",
        "\n",
        "  if data_params['fut_cov_cols'] is not None:\n",
        "    model1.fit(series,\n",
        "               past_covariates = past_cov,\n",
        "               future_covariates = fut_cov,\n",
        "               val_series = val_ser,\n",
        "               val_past_covariates = val_past_cov,\n",
        "               val_future_covariates = val_fut_cov,\n",
        "               callbacks = [lgb.log_evaluation(0)]\n",
        "              )\n",
        "  else:\n",
        "    model1.fit(series,\n",
        "               past_covariates = past_cov,\n",
        "               val_series = val_ser,\n",
        "               val_past_covariates = val_past_cov,\n",
        "               callbacks = [lgb.log_evaluation(0)]\n",
        "              )\n",
        "\n",
        "  return model_tr1, model1\n",
        "\n",
        "\n",
        "def drop_correlated_cols(dataset, threshold=0.95):\n",
        "  '''Adapted from https://stackoverflow.com/a/44674459/100129'''\n",
        "\n",
        "  col_corr = set()  # Set of all the names of deleted columns\n",
        "  corr_matrix = dataset.corr(numeric_only=True).abs()\n",
        "\n",
        "  for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "      if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
        "        colname = corr_matrix.columns[i]\n",
        "        col_corr.add(colname)\n",
        "        if colname in dataset.columns:\n",
        "          del dataset[colname]\n",
        "\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def plot_observation_examples(df, cols, num_plots = 9):\n",
        "    \"\"\"Plot 9 sets of observations in 3 * 3 matrix\"\"\"\n",
        "\n",
        "    num_plots_sqrt = int(np.sqrt(num_plots))\n",
        "    assert num_plots_sqrt ** 2 == num_plots\n",
        "\n",
        "    days = df.ds.dt.date.sample(n = num_plots).sort_values()\n",
        "    p_data = [df[df.ds.dt.date.eq(days[i])] for i in range(num_plots)]\n",
        "\n",
        "    fig, axs = plt.subplots(num_plots_sqrt, num_plots_sqrt, figsize = (15, 10))\n",
        "    axs = axs.ravel()  # apl for the win :-)\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        for col in cols:\n",
        "            axs[i].plot(p_data[i]['ds'], p_data[i][col])\n",
        "            axs[i].xaxis.set_tick_params(rotation = 20, labelsize = 10)\n",
        "\n",
        "    fig.suptitle('Observation examples')\n",
        "    fig.legend(cols, loc = 'lower center',  ncol = len(cols))\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# TODO Change to operate on single dataframe - More useful function :-)\n",
        "#      Change as far as possible - merge(), common_cols etc\n",
        "#      Then write a wrapper to operate on before and after dataframes\n",
        "#      combine results and calculate differences\n",
        "def sanity_check_df_rows_cols_labels(before, after,\n",
        "                                     row_var_cutoff=0.005, col_var_cutoff=0.05,\n",
        "                                     col_corr_cutoff=0.,\n",
        "                                     fast=True, verbose=False):\n",
        "  '''Sanity check dataframes before and after modifications\n",
        "\n",
        "  WARN: default row_var_cutoff, col_var_cutoff, col_corr_cutoff are fairly arbitrary\n",
        "        there is some redundancy between these tests\n",
        "\n",
        "  '''\n",
        "\n",
        "  print_v = print if verbose else lambda *a, **k: None\n",
        "\n",
        "  df = pd.DataFrame(columns = ['before', 'after', 'diff'])\n",
        "  df_labels = []\n",
        "\n",
        "  label = 'rows'\n",
        "  # start_time = timeit.default_timer()\n",
        "  i = 0\n",
        "  df.loc[len(df), df.columns] = before.shape[i], after.shape[i], 0\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'cols'\n",
        "  # start_time = timeit.default_timer()\n",
        "  i = 1\n",
        "  df.loc[len(df), df.columns] = before.shape[i], after.shape[i], 0\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'missing_rows'\n",
        "  # start_time = timeit.default_timer()\n",
        "  i = 0\n",
        "  before_after = pd.merge(before, after, left_index=True, right_index=True, how='outer', indicator=True)\n",
        "  missing_rows = before_after.loc[before_after['_merge'] == 'left_only', :]\n",
        "  df.loc[len(df), df.columns] = 0, missing_rows.shape[i], 0\n",
        "  if missing_rows.shape[i] > 0:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(missing_rows)\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'missing_cols'\n",
        "  # start_time = timeit.default_timer()\n",
        "  i = 1\n",
        "  common_cols = before.columns.intersection(after.columns)\n",
        "  missing_cols = before.shape[i] - len(common_cols)\n",
        "  df.loc[len(df), df.columns] = 0, missing_cols, 0\n",
        "  if missing_cols > 0:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(set(before.columns) - set(common_cols))\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'total_nas'\n",
        "  # start_time = timeit.default_timer()\n",
        "  df.loc[len(df), df.columns] = before.isna().sum().sum(), \\\n",
        "                                after.isna().sum().sum(), 0\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'rows_with_nas'\n",
        "  # start_time = timeit.default_timer()\n",
        "  before_rows_nas = before.isnull().any(axis=1).sum()\n",
        "  after_rows_nas  = after.isnull().any(axis=1).sum()\n",
        "  df.loc[len(df), df.columns] = before_rows_nas, after_rows_nas, 0\n",
        "  if before_rows_nas != after_rows_nas:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(before[before.isnull().any(axis=1)])\n",
        "    print_v(after[after.isnull().any(axis=1)])\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'cols_with_nas'\n",
        "  # start_time = timeit.default_timer()\n",
        "  before_cols_nas = before.isnull().any().sum()\n",
        "  after_cols_nas  = after.isnull().any().sum()\n",
        "  df.loc[len(df), df.columns] = before_cols_nas, after_cols_nas, 0\n",
        "  if before_cols_nas != after_cols_nas:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(before.isnull().any().index.values)\n",
        "    print_v(after.isnull().any().index.values)\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'single_value_rows'\n",
        "  if not fast:\n",
        "    # start_time = timeit.default_timer()\n",
        "    before_single_value_rows = np.sum(before.nunique(axis=1) <= 1)\n",
        "    after_single_value_rows  = np.sum(after.nunique(axis=1) <= 1)\n",
        "    df.loc[len(df), df.columns] = before_single_value_rows, \\\n",
        "                                  after_single_value_rows, 0\n",
        "    if before_single_value_rows != after_single_value_rows:\n",
        "      print_v('\\n', label, ':')\n",
        "      print_v(before[before.nunique(axis=1) <= 1])\n",
        "      print_v(after[after.nunique(axis=1) <= 1])\n",
        "    df_labels.append(label)\n",
        "    # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'single_value_cols'\n",
        "  # start_time = timeit.default_timer()\n",
        "  before_single_value_cols = np.sum(before.nunique() <= 1)\n",
        "  after_single_value_cols  = np.sum(after.nunique() <= 1)\n",
        "  df.loc[len(df), df.columns] = before_single_value_cols, \\\n",
        "                                after_single_value_cols, 0\n",
        "  if before_single_value_cols != after_single_value_cols:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(before.columns[before.nunique() <= 1].values)\n",
        "    print_v(after.columns[after.nunique() <= 1].values)\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  # warnings.resetwarnings()\n",
        "\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "\n",
        "    label = 'low_var_rows'\n",
        "    # start_time = timeit.default_timer()\n",
        "    before_low_var_rows = (before.select_dtypes(include=[np.number]).std(axis=1) <= row_var_cutoff).sum()\n",
        "    after_low_var_rows  = (after.select_dtypes(include=[np.number]).std(axis=1) <= row_var_cutoff).sum()\n",
        "    df.loc[len(df), df.columns] = before_low_var_rows, after_low_var_rows, 0\n",
        "    if before_low_var_rows != after_low_var_rows:\n",
        "      print_v('\\n', label, ':')\n",
        "      print_v(before.select_dtypes(include=[np.number]).std(axis=1) <= row_var_cutoff)\n",
        "      print_v(after.select_dtypes(include=[np.number]).std(axis=1)  <= row_var_cutoff)\n",
        "    df_labels.append(label)\n",
        "    # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "    label = 'low_var_cols'\n",
        "    # start_time = timeit.default_timer()\n",
        "    before_low_var_cols = (before.select_dtypes(include=[np.number]).std() <= col_var_cutoff).sum()\n",
        "    after_low_var_cols  = (after.select_dtypes(include=[np.number]).std() <= col_var_cutoff).sum()\n",
        "    df.loc[len(df), df.columns] = before_low_var_cols, after_low_var_cols, 0\n",
        "    if before_low_var_cols != after_low_var_cols:\n",
        "      print_v('\\n', label, ':')\n",
        "      s = before.select_dtypes(include=[np.number]).std() <= col_var_cutoff\n",
        "      t = after.select_dtypes(include=[np.number]).std()  <= col_var_cutoff\n",
        "      print_v(s[s].index.values)\n",
        "      print_v(t[t].index.values)\n",
        "    df_labels.append(label)\n",
        "    # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'duplicate_rows'\n",
        "  # start_time = timeit.default_timer()\n",
        "  before_dup_rows = before.shape[0] - before.drop_duplicates().shape[0]\n",
        "  after_dup_rows  = after.shape[0]  - after.drop_duplicates().shape[0]\n",
        "  df.loc[len(df), df.columns] = before_dup_rows, after_dup_rows, 0\n",
        "  if before_dup_rows != after_dup_rows:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(before[before.duplicated(keep=False)])\n",
        "    print_v(after[after.duplicated(keep=False)])\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'highly_correlated_cols'\n",
        "  # .copy() so we don't modify the original dataframe\n",
        "  if not fast:\n",
        "    # start_time = timeit.default_timer()\n",
        "    before_high_corr_cols = before.shape[1] - drop_correlated_cols(before.copy(), col_corr_cutoff).shape[1]\n",
        "    after_high_corr_cols  = after.shape[1]  - drop_correlated_cols(after.copy(), col_corr_cutoff).shape[1]\n",
        "    df.loc[len(df), df.columns] = before_high_corr_cols, after_high_corr_cols, 0\n",
        "    if before_high_corr_cols != after_high_corr_cols:\n",
        "      print_v('\\n', label, ':')\n",
        "      print_v(set(before.columns) - set(drop_correlated_cols(before.copy(), col_corr_cutoff).columns))\n",
        "      print_v(set(after.columns) - set(drop_correlated_cols(after.copy(), col_corr_cutoff).columns))\n",
        "    df_labels.append(label)\n",
        "    # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'duplicate_index_labels'\n",
        "  # start_time = timeit.default_timer()\n",
        "  before_idx_labels = before.index.duplicated().sum()\n",
        "  after_idx_labels  = after.index.duplicated().sum()\n",
        "  df.loc[len(df), df.columns] = before_idx_labels, after_idx_labels, 0\n",
        "  if before_idx_labels != after_idx_labels:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(before.index.duplicated())\n",
        "    print_v(after.index.duplicated())\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  label = 'duplicate_col_labels'\n",
        "  # start_time = timeit.default_timer()\n",
        "  before_dup_col_labels = before.columns.duplicated().sum()\n",
        "  after_dup_col_labels  = after.columns.duplicated().sum()\n",
        "  df.loc[len(df), df.columns] = before_dup_col_labels, after_dup_col_labels, 0\n",
        "  if before_dup_col_labels != after_dup_col_labels:\n",
        "    print_v('\\n', label, ':')\n",
        "    print_v(before.columns.duplicated())\n",
        "    print_v(after.columns.duplicated())\n",
        "  df_labels.append(label)\n",
        "  # print('\\t', label, round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  # TODO Find renamed columns from before in after?\n",
        "\n",
        "\n",
        "  df['diff'] = df['after'] - df['before']\n",
        "  df.index = df_labels\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def sanity_check_before_after_dfs(before_, after_, ds_name, fast=True, verbose=False):\n",
        "\n",
        "  print('\\n', ds_name, sep='')\n",
        "\n",
        "  # Reasons I HATE pandas number Inf a neverending series:\n",
        "  # PerformanceWarning: DataFrame is highly fragmented.  This is usually the\n",
        "  # result of calling `frame.insert` many times, which has poor performance.\n",
        "  # Consider joining all columns at once using pd.concat(axis=1) instead. To\n",
        "  # get a de-fragmented frame, use `newframe = frame.copy()`\n",
        "  before = before_.copy()\n",
        "  after  = after_.copy()\n",
        "\n",
        "  # start_time = timeit.default_timer()\n",
        "  sanity_df = sanity_check_df_rows_cols_labels(before, after, fast=fast, verbose=verbose)\n",
        "  # print('\\t sanity_check_df_rows_cols_labels', round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "\n",
        "  # start_time = timeit.default_timer()\n",
        "  print('before.index.equals(after.index):', before.index.equals(after.index))\n",
        "\n",
        "  # check index freq is set and are equal\n",
        "  print('before.index.freq == after.index.freq:', before.index.freq == after.index.freq)\n",
        "  if verbose:\n",
        "    print('before.index.freq:', before.index.freq)\n",
        "    print('after.index.freq:',  after.index.freq)\n",
        "\n",
        "\n",
        "  # check if common column dtypes have changed\n",
        "  common_cols = before.columns.intersection(after.columns)\n",
        "  print('before[common_cols].dtypes == after[common_cols].dtypes:',\n",
        "        (before[common_cols].dtypes == after[common_cols].dtypes).all())\n",
        "  if verbose:\n",
        "    print('before[common_cols].dtypes:', before[common_cols].dtypes)\n",
        "    print('after[common_cols].dtypes:',  after[common_cols].dtypes)\n",
        "\n",
        "  # check if describe() summaries are equal\n",
        "  print('before[common_cols].describe() == after[common_cols].describe():',\n",
        "        (before[common_cols].describe() == after[common_cols].describe()).all().all())\n",
        "  if verbose:\n",
        "    print(before[common_cols].describe() == after[common_cols].describe())\n",
        "\n",
        "  # check after subsetted by before equals before\n",
        "  print('\\nbefore[common_cols].equals(after[common_cols]):',\n",
        "  before[common_cols].dropna().drop_duplicates().equals(after[common_cols].dropna().drop_duplicates())\n",
        "  )\n",
        "  if verbose:\n",
        "    print('before.isin(after):',\n",
        "    before[common_cols].dropna().drop_duplicates().isin(after[common_cols].dropna().drop_duplicates()).all().all()\n",
        "    )\n",
        "    print(before.dropna().drop_duplicates().isin(after.dropna().drop_duplicates()).all())\n",
        "    print(before.dropna().drop_duplicates().isin(after.dropna().drop_duplicates()))\n",
        "\n",
        "\n",
        "  # Reasons I HATE pandas number Inf a neverending series:\n",
        "  # PerformanceWarning: DataFrame is highly fragmented.  This is usually the\n",
        "  # result of calling `frame.insert` many times, which has poor performance.\n",
        "  # Consider joining all columns at once using pd.concat(axis=1) instead. To\n",
        "  # get a de-fragmented frame, use `newframe = frame.copy()`\n",
        "  # calculate duplicate row counts then find mean duplicate count\n",
        "  # for each column and finally find mean of means aka redundancy\n",
        "  # warnings.resetwarnings()\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    before_red = before.dropna().groupby(before.select_dtypes(include=np.number).columns.tolist(), as_index=False).size().mean().mean()\n",
        "    after_red  = after.dropna().groupby(after.select_dtypes(include=np.number).columns.tolist(), as_index=False).size().mean().mean()\n",
        "    print('redundancy before > after:', before_red > after_red)\n",
        "    print('mean before feature redundancy:', round(before_red, 3))\n",
        "    print('mean after feature redundancy: ', round(after_red,  3))\n",
        "\n",
        "  # Check all data is numeric, finite (but allow NAs) and reasonably shaped\n",
        "  # If any problems then this will error out\n",
        "  # Only checking 'after' dataframe\n",
        "  # https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_X_y.html\n",
        "  if Y_COL in after.columns:\n",
        "    _, _ = check_X_y(after.drop(columns=[Y_COL, 'ds']),\n",
        "                     after[Y_COL],\n",
        "                     y_numeric = True,\n",
        "                     force_all_finite = 'allow-nan')\n",
        "\n",
        "  print()\n",
        "  # print('\\t end sanity_check_before_after_dfs', round(timeit.default_timer() - start_time, 2))\n",
        "\n",
        "  display(sanity_df)\n",
        "\n",
        "  return sanity_df\n",
        "\n",
        "\n",
        "def compare_train_valid_test_sanity_dfs(train_sanity, valid_sanity, test_sanity, ex_labels=None):\n",
        "  '''...'''\n",
        "\n",
        "  if ex_labels is None:\n",
        "    ex_labels = ['rows']\n",
        "\n",
        "  train_sanity = train_sanity.loc[~train_sanity.index.isin(ex_labels)]\n",
        "  valid_sanity = valid_sanity.loc[~valid_sanity.index.isin(ex_labels)]\n",
        "  test_sanity  =  test_sanity.loc[~test_sanity.index.isin(ex_labels)]\n",
        "\n",
        "  if not train_sanity.equals(valid_sanity):\n",
        "    print('WARN: train_sanity != valid_sanity')\n",
        "    display(pd.concat([train_sanity, valid_sanity]).drop_duplicates(keep=False))\n",
        "\n",
        "  if not train_sanity.equals(test_sanity):\n",
        "    print('WARN: train_sanity != test_sanity')\n",
        "    display(pd.concat([train_sanity, test_sanity]).drop_duplicates(keep=False))\n",
        "\n",
        "  if not test_sanity.equals(valid_sanity):\n",
        "    print('WARN: test_sanity != valid_sanity')\n",
        "    display(pd.concat([test_sanity, valid_sanity]).drop_duplicates(keep=False))\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "# TODO Remove some of the code duplication\n",
        "def sanity_check_train_valid_test(train_df, valid_df, test_df,\n",
        "                                  over_cols = ['y_des', 'dew.point_des', 'humidity', 'pressure'],\n",
        "                                  dp = 2):\n",
        "\n",
        "  # Check number of columns is equal\n",
        "  if (train_df.shape[1] != valid_df.shape[1]) or \\\n",
        "     (train_df.shape[1] != test_df.shape[1])  or \\\n",
        "     (valid_df.shape[1] != test_df.shape[1]):\n",
        "    print('ERROR: Inconsistent number of columns!')\n",
        "    print('train_df.shape[1]:', train_df.shape[1])\n",
        "    print('valid_df.shape[1]:', valid_df.shape[1])\n",
        "    print('test_df.shape[1]:',  test_df.shape[1])\n",
        "\n",
        "\n",
        "  # Check column names are equal\n",
        "  if not (train_df.columns == valid_df.columns).all():\n",
        "    print('ERROR: Inconsistent train_df, valid_df column names!')\n",
        "    print('train_df.columns:', train_df.columns)\n",
        "    print('valid_df.columns:', valid_df.columns)\n",
        "\n",
        "  if not (train_df.columns == test_df.columns).all():\n",
        "    print('ERROR: Inconsistent train_df, test_df column names!')\n",
        "    print('train_df.columns:', train_df.columns)\n",
        "    print('test_df.columns:',  test_df.columns)\n",
        "\n",
        "  if not (valid_df.columns == test_df.columns).all():\n",
        "    print('ERROR: Inconsistent valid_df, test_df column names!')\n",
        "    print('valid_df.columns:', valid_df.columns)\n",
        "    print('test_df.columns:',  test_df.columns)\n",
        "\n",
        "\n",
        "  # Check column dtypes are equal\n",
        "  if not (train_df.dtypes == valid_df.dtypes).all():\n",
        "    print('ERROR: Inconsistent train_df, valid_df dtypes!')\n",
        "    print('train_df.dtypes:', train_df.dtypes)\n",
        "    print('valid_df.dtypes:', valid_df.dtypes)\n",
        "\n",
        "  if not (train_df.dtypes == test_df.dtypes).all():\n",
        "    print('ERROR: Inconsistent train_df, test_df dtypes!')\n",
        "    print('train_df.dtypes:', train_df.dtypes)\n",
        "    print('test_df.dtypes:',  test_df.dtypes)\n",
        "\n",
        "  if not (valid_df.dtypes == test_df.dtypes).all():\n",
        "    print('ERROR: Inconsistent valid_df, test_df dtypes!')\n",
        "    print('valid_df.dtypes:', valid_df.dtypes)\n",
        "    print('test_df.dtypes:',  test_df.dtypes)\n",
        "\n",
        "\n",
        "  # Check index freqs are equal\n",
        "  if train_df.index.freq != valid_df.index.freq:\n",
        "    print('ERROR: Inconsistent train_df, valid_df index frequencies!')\n",
        "    print('train_df.index.freq:', train_df.index.freq)\n",
        "    print('valid_df.index.freq:', valid_df.index.freq)\n",
        "\n",
        "  if train_df.index.freq != test_df.index.freq:\n",
        "    print('ERROR: Inconsistent train_df, test_df index frequencies!')\n",
        "    print('train_df.index.freq:', train_df.index.freq)\n",
        "    print('test_df.index.freq:',   test_df.index.freq)\n",
        "\n",
        "  if valid_df.index.freq != test_df.index.freq:\n",
        "    print('ERROR: Inconsistent valid_df, test_df index frequencies!')\n",
        "    print('valid_df.index.freq:', valid_df.index.freq)\n",
        "    print('test_df.index.freq:',   test_df.index.freq)\n",
        "\n",
        "\n",
        "  # Verify dataframes are different!\n",
        "  if train_df.equals(valid_df):\n",
        "    print('ERROR: train_df == valid_df!')\n",
        "\n",
        "  if train_df.equals(test_df):\n",
        "    print('ERROR: train_df == test_df!')\n",
        "\n",
        "  if valid_df.equals(test_df):\n",
        "    print('ERROR: valid_df == test_df!')\n",
        "\n",
        "\n",
        "  # Check no overlap between train_df.index and valid_df.index\n",
        "  # train_df.index strictly before valid_df.index and test_df.index\n",
        "  if max(train_df.index) >= min(valid_df.index):\n",
        "    print('ERROR: Overlap between train_df, valid_df indices!')\n",
        "    print('max(train_df.index):', max(train_df.index))\n",
        "    print('min(valid_df.index):', max(valid_df.index))\n",
        "\n",
        "  # Check no overlap between train_df.index and test_df.index\n",
        "  # train_df.index strictly before valid_df.index and test_df.index\n",
        "  if max(train_df.index) >= min(test_df.index):\n",
        "    print('ERROR: Overlap between train_df, test_df indices!')\n",
        "    print('max(train_df.index):', max(train_df.index))\n",
        "    print('min(test_df.index):',  max(test_df.index))\n",
        "\n",
        "\n",
        "  # Check no overlap between valid_df.index and test_df.index\n",
        "  # valid_df.index can be before or after test_df.index\n",
        "  if (max(valid_df.index) >= min(test_df.index)) and \\\n",
        "     (max(valid_df.index) <= max(test_df.index)):\n",
        "    print('ERROR: Overlap between valid_df, test_df indices!')\n",
        "    print('valid_df.index:', max(valid_df.index), '-', max(valid_df.index))\n",
        "    print('test_df.index:',  max(test_df.index),  '-', max(test_df.index))\n",
        "\n",
        "  if (min(valid_df.index) >= min(test_df.index)) and \\\n",
        "     (min(valid_df.index) <= max(test_df.index)):\n",
        "    print('ERROR: Overlap between valid_df, test_df indices!')\n",
        "    print('valid_df.index:', max(valid_df.index), '-', max(valid_df.index))\n",
        "    print('test_df.index:',  max(test_df.index),  '-', max(test_df.index))\n",
        "\n",
        "\n",
        "  # TODO: Consider enforcing a gap of 1 day to 1 week between\n",
        "  #       train_df.index and {valid_df,test_df}.index to avoid data leakage?\n",
        "\n",
        "\n",
        "  # Check train_df has more observations than valid_df and test_df\n",
        "  if valid_df.shape[0] > train_df.shape[0]:\n",
        "    print('ERROR: valid_df more observations than train_df!')\n",
        "    print('train_df observations:', train_df.shape[0])\n",
        "    print('valid_df observations:', valid_df.shape[0])\n",
        "\n",
        "  if test_df.shape[0] > train_df.shape[0]:\n",
        "    print('ERROR: test_df more observations than train_df!')\n",
        "    print('train_df observations:', train_df.shape[0])\n",
        "    print('test_df observations:',  test_df.shape[0])\n",
        "\n",
        "\n",
        "  # Check valid_df and test_df have equal number of observations\n",
        "  # valid_df and test_df may be different sizes but\n",
        "  # large size difference may indicate an issue\n",
        "  # TODO: Use calendar.isleap() to check if leap year\n",
        "  if valid_df.shape[0] != test_df.shape[0]:\n",
        "    print('WARN: Inconsistent number of valid_df, test_df rows.  Leap year?')\n",
        "\n",
        "\n",
        "  # Check valid_df and test_df are each 1 year long\n",
        "  YEAR_OBS_MIN = 48 * 365\n",
        "  YEAR_OBS_MAX = 48 * 366\n",
        "  if (valid_df.shape[0] < YEAR_OBS_MIN) or \\\n",
        "     (valid_df.shape[0] > YEAR_OBS_MAX):\n",
        "    print('ERROR: valid_df should be 1 year long [',\n",
        "          YEAR_OBS_MIN, ',', YEAR_OBS_MAX, ']!')\n",
        "    print('valid_df observations:', valid_df.shape[0])\n",
        "\n",
        "  if (test_df.shape[0] < YEAR_OBS_MIN) or \\\n",
        "     (test_df.shape[0] > YEAR_OBS_MAX):\n",
        "    print('ERROR: test_df should be 1 year long [',\n",
        "          YEAR_OBS_MIN, ',', YEAR_OBS_MAX, ']!')\n",
        "    print('test_df observations:', test_df.shape[0])\n",
        "\n",
        "  # Check approx number of overlapping rows between train_df and valid_df\n",
        "  dups_pc_lim = 15.0\n",
        "  n_dups, dups_pc = get_approx_overlap(train_df, valid_df, over_cols, decs=dp)\n",
        "  if dups_pc > dups_pc_lim:\n",
        "    print('WARN: high overlap between train_df and valid_df rows!')\n",
        "    print(f\"Number of shared rows: {n_dups}\")\n",
        "    print(f'Approximate overlap: {dups_pc} %\\n')\n",
        "    # print(f'Decimal places: {dp}')\n",
        "    # print('Overlap features:', over_cols)\n",
        "\n",
        "  # Check approx number of overlapping rows between train_df and test_df\n",
        "  n_dups, dups_pc = get_approx_overlap(train_df, test_df, over_cols, decs=dp)\n",
        "  if dups_pc > dups_pc_lim:\n",
        "    print('WARN: high overlap between train_df and test_df rows!')\n",
        "    print(f\"Number of shared rows: {n_dups}\")\n",
        "    print(f'Approximate overlap: {dups_pc} %\\n')\n",
        "    # print(f'Decimal places: {dp}')\n",
        "    # print('Overlap features:', over_cols)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def print_train_valid_test_shapes(df, train_df, valid_df, test_df):\n",
        "  print(\"df shape: \",            df.shape)\n",
        "  print(\"train shape:   \", train_df.shape)\n",
        "  print(\"valid shape:   \", valid_df.shape)\n",
        "  print(\"test shape:    \",  test_df.shape)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def plot_feature_history_single_df(data, var, missing=False):\n",
        "    plt.figure(figsize = (12, 6))\n",
        "    plt.scatter(data.index, data[var],\n",
        "                label='train', color='black', s=3)\n",
        "    if missing:\n",
        "      label = 'missing'\n",
        "      x_lab = data.loc[data[label] == 1.0, 'ds']\n",
        "      y_lab = data.loc[data[label] == 1.0, var]\n",
        "      plt.scatter(x_lab, y_lab, color='red', label=label, s=3)\n",
        "\n",
        "    plt.title(var)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_feature_history(train, valid, test, var, missing=False):\n",
        "    label = 'missing'\n",
        "\n",
        "    plt.figure(figsize = (12, 6))\n",
        "    plt.scatter(train.index, train[var],\n",
        "                label='train', color='black', s=3)\n",
        "    if missing:\n",
        "      x_lab = train.loc[train[label] == 1.0, 'ds']\n",
        "      y_lab = train.loc[train[label] == 1.0, var]\n",
        "      plt.scatter(x_lab, y_lab, color='red', label=label, s=3)\n",
        "\n",
        "    plt.scatter(valid.index, valid[var],\n",
        "                label='valid', color='blue', s=3)\n",
        "    if missing:\n",
        "      x_lab = valid.loc[valid[label] == 1.0, 'ds']\n",
        "      y_lab = valid.loc[valid[label] == 1.0, var]\n",
        "      plt.scatter(x_lab, y_lab, color='red', label=label, s=3)\n",
        "\n",
        "    plt.scatter(test.index,  test[var],\n",
        "                label='test', color='purple', s=3)\n",
        "    if missing:\n",
        "      x_lab = test.loc[test[label] == 1.0, 'ds']\n",
        "      y_lab = test.loc[test[label] == 1.0, var]\n",
        "      plt.scatter(x_lab, y_lab, color='red', label=label, s=3)\n",
        "\n",
        "    plt.title(var)\n",
        "    #ax = plt.gca()\n",
        "    #leg = ax.get_legend()\n",
        "    #leg.legendHandles[0].set_color('black')\n",
        "    #leg.legendHandles[1].set_color('red')\n",
        "    #leg.legendHandles[2].set_color('blue')\n",
        "    #leg.legendHandles[3].set_color('red')\n",
        "    #leg.legendHandles[4].set_color('purple')\n",
        "    #leg.legendHandles[5].set_color('red')\n",
        "    #hl_dict = {handle.get_label(): handle for handle in leg.legendHandles}\n",
        "    #hl_dict['train'].set_color('black')\n",
        "    #hl_dict['valid'].set_color('blue')\n",
        "    #hl_dict['test'].set_color('purple')\n",
        "    #hl_dict[label].set_color('red')\n",
        "    #plt.legend(['train', 'valid', 'test', label])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_feature_history_separately(train, valid, test, var):\n",
        "    fig, axs = plt.subplots(1, 3, figsize = (14, 7))\n",
        "\n",
        "    axs[0].plot(train.index, train[var])\n",
        "    axs[0].set_title('train')\n",
        "\n",
        "    axs[1].plot(valid.index, valid[var])\n",
        "    axs[1].set_title('valid')\n",
        "    axs[1].set_xticks(axs[1].get_xticks(), axs[1].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    axs[2].plot(test.index,  test[var])\n",
        "    axs[2].set_title('test')\n",
        "    axs[2].set_xticks(axs[2].get_xticks(), axs[2].get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    fig.suptitle(var)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def check_high_low_thresholds(df, ds=None):\n",
        "  '''Check main features from dataframe are within reasonable thresholds'''\n",
        "\n",
        "  all_ok = True\n",
        "  feats = ['y', 'dew.point', 'humidity', 'pressure',\n",
        "           'wind.speed.mean', 'wind.speed.max']\n",
        "  highs = [ 45,  25, 100, 1060, 35, 70]\n",
        "  lows  = [-20, -20,   5,  950,  0,  0]\n",
        "\n",
        "  thresh = pd.DataFrame({'feat': feats,\n",
        "                         'high': highs,\n",
        "                         'low':  lows,})\n",
        "  thresh.index = feats\n",
        "\n",
        "  for feat in feats:\n",
        "    feat_high = thresh.loc[feat, 'high']\n",
        "    feat_low  = thresh.loc[feat, 'low']\n",
        "\n",
        "    if not df[feat].between(feat_low, feat_high).all():\n",
        "      all_ok = False\n",
        "      print('%15s [%3d, %3d] - % 7.3f, % 7.3f' %\n",
        "            (feat, feat_low, feat_high,\n",
        "            round(min(df[feat]), 3), round(max(df[feat]), 3)))\n",
        "\n",
        "  # check if dew.point ever greater than temperature\n",
        "  if df.loc[df['dew.point'] > df['y'], ['y', 'dew.point']].shape[0] != 0:\n",
        "    all_ok = False\n",
        "    print('dew.point > y:')\n",
        "    display(df.loc[df['dew.point'] > df['y'], ['y', 'dew.point']])\n",
        "\n",
        "  if all_ok is False:\n",
        "    print(' ... from', ds)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_features_filename(feat_name, data_name, date_str, file_ext='.csv.xz'):\n",
        "    return feat_name + data_name + date_str + file_ext\n",
        "\n",
        "\n",
        "def merge_data_and_aggs(data, aggs):\n",
        "  data = pd.concat((data, aggs), axis=1)\n",
        "  # data = data.join(aggs)\n",
        "\n",
        "  # data.set_index('ds', drop = False, inplace = True)\n",
        "  data['ds'] = data.index\n",
        "  data = data[~data.index.duplicated(keep = 'first')]\n",
        "  data = data.asfreq(freq = '30min')\n",
        "\n",
        "  # Reasons I HATE pandas number Inf a neverending series:\n",
        "  # PerformanceWarning: DataFrame is highly fragmented.  This is usually the\n",
        "  # result of calling `frame.insert` many times, which has poor performance.\n",
        "  # Consider joining all columns at once using pd.concat(axis=1) instead. To\n",
        "  # get a de-fragmented frame, use `newframe = frame.copy()`\n",
        "  # data_ = data.copy()\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def print_null_columns(df, df_name):\n",
        "  print('\\n', df_name, 'null columns:')\n",
        "  display(df[df.columns[df.isnull().any()]].isnull().sum())\n",
        "\n",
        "\n",
        "def print_na_locations(df):\n",
        "  '''Print index row and column labels for NA in dataframe'''\n",
        "\n",
        "  for index, row in df[df.isna().any(axis=1)].items():\n",
        "    for col_name, row_item in row.items():\n",
        "      if pd.isnull(df.loc[index, col_name]):\n",
        "        print(index, col_name)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "def get_darts_series(data, data_params):\n",
        "  series   = TimeSeries.from_dataframe(data, value_cols=data_params['y_col'])\n",
        "  past_cov = TimeSeries.from_dataframe(data, value_cols=data_params['past_cov_cols'])\n",
        "\n",
        "  if data_params['fut_cov_cols'] is not None:\n",
        "    fut_cov = TimeSeries.from_dataframe(data, value_cols=data_params['fut_cov_cols'])\n",
        "  else:\n",
        "    fut_cov = None\n",
        "\n",
        "  return series, past_cov, fut_cov\n",
        "\n",
        "\n",
        "def plot_short_term_acf(data, acf_feats, acf_cols, title_,\n",
        "                        mean_feat = False, max_lags = 300):\n",
        "  plt.figure(figsize = (12, 6))\n",
        "\n",
        "  acf = pd.DataFrame()\n",
        "\n",
        "  for acf_feat, acf_col in zip(acf_feats, acf_cols):\n",
        "    acf[acf_feat] = [data[acf_feat].autocorr(l) for l in range(1, max_lags)]\n",
        "    plt.plot(acf[acf_feat], label=acf_feat, c=acf_col)\n",
        "\n",
        "  if mean_feat:\n",
        "    acf['mean_acf'] = acf.mean(axis=1)\n",
        "    plt.plot(acf['mean_acf'], label='mean_acf', c='black')\n",
        "\n",
        "  plt.axhline(0, linestyle='--', c='black')\n",
        "  plt.axhline(0.875, linestyle=':', c='lightgrey')\n",
        "  plt.ylabel('autocorrelation')\n",
        "  plt.xlabel('time lags')\n",
        "  plt.title(title_)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_long_term_acf(data, var, num_years=3):\n",
        "  # WARN: Slow function :-(\n",
        "  #       Results are more useful when displaying more years of data\n",
        "  pd.plotting.autocorrelation_plot(data[var].head(17532 * num_years))\n",
        "  plt.title(var)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def print_df_summary(df):\n",
        "  print(\"Shape:\")\n",
        "  display(df.shape)\n",
        "\n",
        "  total_nas = df.isna().sum().sum()\n",
        "  rows_nas  = df.isnull().any(axis=1).sum()\n",
        "  cols_nas  = df.isnull().any().sum()\n",
        "  print('\\nTotal NAs:', total_nas)\n",
        "  print('Rows with NAs:', rows_nas)\n",
        "  print('Cols with NAs:', cols_nas)\n",
        "\n",
        "  print(\"\\nInfo:\")\n",
        "  display(df.info())\n",
        "\n",
        "  print(\"\\nSummary stats:\")\n",
        "  display(df.describe())\n",
        "\n",
        "  print(\"\\nRaw data:\")\n",
        "  display(df)\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "def get_approx_overlap(X1, X2, over_cols, decs=2, verbose=False):\n",
        "  '''Calculate approximate overlap between 2 dataframes of different sizes.\n",
        "\n",
        "  If exact values are used then overlap is probably too low,\n",
        "  so use np.round() to reduce precision.\n",
        "  Use MinMaxScaler so single decimals parameter is applicable to all columns.\n",
        "  Assumes X1 is train and X2 is valid/test.\n",
        "  Duplicates dropped from X1 & X2 before calculating overlap.\n",
        "  Percent overlap can be greater than 100 if decs is too low.\n",
        "\n",
        "  Based on https://stackoverflow.com/a/71002234/100129\n",
        "  '''\n",
        "\n",
        "  assert X1.shape[0] >= X2.shape[0]\n",
        "\n",
        "  X1 = X1[over_cols].drop_duplicates()\n",
        "  X2 = X2[over_cols].drop_duplicates()\n",
        "\n",
        "  Xcomb = pd.concat((X1, X2), axis=0, ignore_index=True)\n",
        "\n",
        "  # scale\n",
        "  scaler = MinMaxScaler()\n",
        "  Xscl = scaler.fit_transform(Xcomb)\n",
        "\n",
        "  # round\n",
        "  # df_scl = pd.DataFrame(np.round(Xcomb, decimals=decs), columns=over_cols)\n",
        "  df_scl = pd.DataFrame(np.round(Xscl, decimals=decs), columns=over_cols)\n",
        "\n",
        "  # count overlaps\n",
        "  n_uniq = df_scl.drop_duplicates().shape[0]\n",
        "  n_dup = X1.shape[0] + X2.shape[0] - n_uniq\n",
        "  dup_pc = round(n_dup * 100 / X2.shape[0], 2)\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"Number of shared rows: {n_dup}\")\n",
        "    print(f'Approximate overlap: {dup_pc} %\\n')\n",
        "\n",
        "  if dup_pc > 100.0:\n",
        "    print('Approx. overlap over 100 %!')\n",
        "    print('Increase decs argument')\n",
        "    print(f\"decs = {decs}\\n\")\n",
        "\n",
        "  return n_dup, dup_pc\n",
        "\n",
        "\n",
        "def add_transform_column(mod_vi_df, us_feats):\n",
        "  mod_vi_df['transform'] = 'None'\n",
        "\n",
        "  ne_mask = mod_vi_df['feature_transform'] != mod_vi_df['feature']\n",
        "  ne_fs = mod_vi_df.loc[ne_mask, 'feature'].to_list()\n",
        "  ne_fts = mod_vi_df.loc[ne_mask, 'feature_transform'].to_list()\n",
        "  # display(mod_vi_df.loc[ne_mask, ['feature', 'feature_transform']])\n",
        "  # print('ne_fs:', ne_fs)\n",
        "  # print('ne_fts:', ne_fts)\n",
        "\n",
        "  ne_ts = [ne_ft.replace(ne_fs[i] + '_', '') for i, ne_ft in enumerate(ne_fts)]\n",
        "\n",
        "  mod_vi_df.loc[ne_mask, 'transform'] = ne_ts\n",
        "\n",
        "  for us_feat in us_feats:\n",
        "    if us_feat.endswith('_des'):\n",
        "      # 'y_des', 'dew.point_des'\n",
        "      mod_vi_df.loc[mod_vi_df['feature_transform'] == us_feat, 'transform'] = 'des'\n",
        "    else:\n",
        "      # 't_pot', 'vp_def', 'air_density', 'ground_hf', 'za_rad', ...\n",
        "      mod_vi_df.loc[mod_vi_df['feature'] == us_feat, 'transform'] = 'None'\n",
        "\n",
        "  wind_mask = mod_vi_df['feature_transform_lag'].str.contains('_window_')\n",
        "  mod_vi_df.loc[wind_mask, 'transform'] = mod_vi_df.loc[wind_mask, 'feature_transform_lag'].str.extract(r'_window_\\d+_(.*)_[target|pastcov|futcov]', expand=False)\n",
        "\n",
        "  return mod_vi_df\n",
        "\n",
        "\n",
        "def add_feature_column(mod_vi_df, us_feats):\n",
        "  mod_vi_df['feature'] = ''\n",
        "\n",
        "  # extract features which DO NOT contain underscores\n",
        "  # eg. irradiance, pressure ...\n",
        "  useq0_mask = mod_vi_df['feature_transform'].str.count('_') == 0\n",
        "  mod_vi_df.loc[useq0_mask, 'feature'] = mod_vi_df.loc[useq0_mask, 'feature_transform']\n",
        "  # print('useq0:')\n",
        "  # display(mod_vi_df.loc[useq0_mask, 'feature'])\n",
        "\n",
        "\n",
        "  # extract features which DO contain underscores\n",
        "  # cannot distinguish between y_des and pressure_grad\n",
        "  useq1_mask = mod_vi_df['feature_transform'].str.count('_') == 1\n",
        "  # pandas, for clowns by clowns - expand=False your arse (at least an hour wasted)\n",
        "  mod_vi_df.loc[useq1_mask, 'feature'] = mod_vi_df.loc[useq1_mask, 'feature_transform'].str.extract(r'^(.*)_', expand=False)\n",
        "  # print('useq1:')\n",
        "  # display(mod_vi_df.loc[useq1_mask, 'feature'])\n",
        "\n",
        "  # cannot distinguish between y_des_hist_mode and pressure_intervals_intervals_mean\n",
        "  usgt1_mask = mod_vi_df['feature_transform'].str.count('_') > 1\n",
        "  # pandas, for clowns by clowns - expand=False your arse (at least an hour wasted)\n",
        "  mod_vi_df.loc[usgt1_mask, 'feature'] = mod_vi_df.loc[usgt1_mask, 'feature_transform'].str.extract(r'^([a-zA-Z0-9\\.]+_[a-zA-Z0-9\\.]+)_', expand=False)\n",
        "  # print('usgt1:')\n",
        "  # display(mod_vi_df.loc[usgt1_mask, 'feature'])\n",
        "\n",
        "  # try and fix cannot distinguish problems mentioned above\n",
        "  for us_feat in us_feats:\n",
        "    mod_vi_df.loc[mod_vi_df['feature_transform'] == us_feat, 'feature'] = us_feat\n",
        "\n",
        "  wind_mask = mod_vi_df['feature_transform_lag'].str.contains('_window_')\n",
        "  mod_vi_df.loc[wind_mask, 'feature'] = mod_vi_df.loc[wind_mask, 'feature_transform_lag'].str.extract(r'^(.*?)_window_\\d+', expand=False)\n",
        "\n",
        "  return mod_vi_df\n",
        "\n",
        "\n",
        "def add_shadow_imp_column(mod_vi_df):\n",
        "  shad_imp = mod_vi_df.loc[mod_vi_df['feature'] == 'y_des_shadow', ['feature', 'model', 'lag', 'imp']]\n",
        "  shad_imp = shad_imp.rename(columns={'imp': 'shadow_imp'})\n",
        "\n",
        "  mod_vi_si = pd.merge(mod_vi_df, shad_imp, how='outer', left_on=['model', 'lag'], right_on=['model', 'lag'])\n",
        "  mod_vi_si.drop('feature_y', axis=1, inplace=True)\n",
        "  mod_vi_si.rename(columns={'feature_x': 'feature'}, inplace=True)\n",
        "  mod_vi_si['shadow_imp'] = mod_vi_si['shadow_imp'].fillna(0)\n",
        "  mod_vi_si['shadow_imp'] = mod_vi_si['shadow_imp'].astype('int')\n",
        "\n",
        "  return mod_vi_si\n",
        "\n",
        "\n",
        "def add_feature_window_transform_column(mod_vi_df):\n",
        "  mod_vi_df['feature_window_transform'] = mod_vi_df['feature_transform_lag']\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    mod_vi_df['feature_window_transform'] = mod_vi_df['feature_window_transform'].str.replace('_lag.*', '', regex=True)\n",
        "    mod_vi_df['feature_window_transform'] = mod_vi_df['feature_window_transform'].str.replace('_(target|pastcov|futcov)', '', regex=True)\n",
        "    mod_vi_df['feature_window_transform'] = mod_vi_df['feature_window_transform'].str.replace('_shift_\\d+', '', regex=True)\n",
        "\n",
        "  return mod_vi_df\n",
        "\n",
        "\n",
        "def add_feature_transform_column(mod_vi_df):\n",
        "  mod_vi_df['feature_transform'] = mod_vi_df['feature_window_transform']\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    mod_vi_df['feature_transform'] = mod_vi_df['feature_transform'].str.replace('_window_\\d+', '', regex=True)\n",
        "\n",
        "  # print('feature_transform:')\n",
        "  # display(mod_vi_df.loc[mod_vi_df['feature_transform'].isna(), 'feature_transform'])\n",
        "\n",
        "  return mod_vi_df\n",
        "\n",
        "\n",
        "def get_multi_model_feat_imps(mmodel, horizon=HORIZON, verbose=False):\n",
        "  mod_vi_list = []\n",
        "\n",
        "  for i in range(horizon):\n",
        "    mod_vi = pd.DataFrame({\n",
        "        'model': i,\n",
        "        'feature_transform_lag': mmodel.lagged_feature_names,\n",
        "        'imp': mmodel.get_multioutput_estimator(i, 0).feature_importances_})\n",
        "    mod_vi_list.append(mod_vi)\n",
        "\n",
        "  mod_vi_df = pd.concat(mod_vi_list)\n",
        "  mod_vi_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # should have been fixed upstream - my bad\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    mod_vi_df['feature_transform_lag'] = mod_vi_df['feature_transform_lag'].str.replace(r'^_(.*?)_\\1', r'_\\1', regex=True)\n",
        "    # mod_vi_df['feature_transform_lag'] = mod_vi_df['feature_transform_lag'].str.replace('intervals_intervals', 'intervals')\n",
        "\n",
        "  mod_vi_df = add_feature_window_transform_column(mod_vi_df)\n",
        "  mod_vi_df = add_feature_transform_column(mod_vi_df)\n",
        "\n",
        "  # us_feats - feature names which contain underscores\n",
        "  us_feats = ['y_des', 'dew.point_des', 't_pot', 'vp_def', 'air_density',\n",
        "              'ground_hf', 'za_rad', 'azimuth_cos', 'azimuth_sin',\n",
        "              'rain_prev_6_hours', 'rain_prev_12_hours', 'rain_prev_24_hours',\n",
        "              'rain_prev_24_hours_binary', 'rain_prev_48_hours',\n",
        "              'rain_prev_48_hours_binary', 'mixing_ratio', 'specific_humidity',\n",
        "              'vapour_pressure',]\n",
        "  mod_vi_df = add_feature_column(mod_vi_df, us_feats)\n",
        "  mod_vi_df = add_transform_column(mod_vi_df, us_feats)\n",
        "\n",
        "  mod_vi_df['lag']  = mod_vi_df['feature_transform_lag'].str.extract('_lag(-?\\d+)$').fillna(0).astype(int)\n",
        "  mod_vi_df['type'] = mod_vi_df['feature_transform_lag'].str.extract('_(target|pastcov|futcov)_').fillna('')\n",
        "  mod_vi_df['shift']  = mod_vi_df['feature_transform_lag'].str.extract('_shift_(\\d+)_').fillna(0).astype(int)\n",
        "  mod_vi_df['window'] = mod_vi_df['feature_transform_lag'].str.extract('_window_(\\d+)_').fillna(0).astype(int)\n",
        "\n",
        "  # very slow (2 or 3 mins) :-(\n",
        "  mod_vi_df = get_above_between_below_feats_with_model_lag(mod_vi_df, ['dew.point_des', 'pressure', 'humidity'], 'cf')\n",
        "  # mod_vi_df = get_above_between_below_feats_with_model_lag(mod_vi_df, ['irradiance', 'za_rad'], 'sf')\n",
        "  # mod_vi_df = get_above_between_below_feats_with_model_lag(mod_vi_df, ['y_des_shadow'], 'shadow')\n",
        "\n",
        "  mod_vi_si = add_shadow_imp_column(mod_vi_df)\n",
        "\n",
        "  mod_vi_si['shadow_geq'] = 0\n",
        "  mod_vi_si.loc[mod_vi_si['shadow_imp'] >= mod_vi_si['imp'], 'shadow_geq'] = 1\n",
        "\n",
        "  if verbose:\n",
        "    display(mod_vi_si)\n",
        "\n",
        "  return mod_vi_si\n",
        "\n",
        "\n",
        "# WARN Very slow - 2 or 3 mins :-(\n",
        "# TODO Speed it up!\n",
        "def get_above_between_below_feats_with_model_lag(fs, feats, feat_str, imp='imp'):\n",
        "  # cf - core features\n",
        "  # sf - solar features\n",
        "\n",
        "  fs['above_' + feat_str] = 0\n",
        "  fs['below_' + feat_str] = 0\n",
        "  if len(feats) > 1:\n",
        "      fs['between_' + feat_str] = 0\n",
        "\n",
        "  # groupby model lag feature\n",
        "  fs_gb = fs[['model', 'lag', 'feature', imp]].groupby(['model', 'lag', 'feature'])\n",
        "\n",
        "  for name, groups in fs_gb:\n",
        "    # print(f'{name = }')\n",
        "    model, lag, feature = name\n",
        "    # print(model, lag, feature)\n",
        "    min_score = np.min(fs.loc[(fs['model'] == model) & (fs['lag'] == lag) & (fs['feature'].isin(feats)), imp])\n",
        "    max_score = np.max(fs.loc[(fs['model'] == model) & (fs['lag'] == lag) & (fs['feature'].isin(feats)), imp])\n",
        "\n",
        "    fs.loc[(fs['model'] == model) & (fs['lag'] == lag) & (fs[imp] > max_score), 'above_' + feat_str] = 1\n",
        "    fs.loc[(fs['model'] == model) & (fs['lag'] == lag) & (fs[imp] < min_score), 'below_' + feat_str] = 1\n",
        "\n",
        "    if len(feats) > 1:\n",
        "      fs.loc[(fs['model'] == model) & (fs['lag'] == lag) & (fs[imp] >= min_score) & (fs[imp] <= max_score), 'between_' + feat_str] = 1\n",
        "      # fs.loc[(fs['model'] == model) & (fs['lag'] == lag) & (fs['feature'].isin(feats)), 'between_' + feat_str] = 1\n",
        "\n",
        "  return fs\n",
        "\n",
        "\n",
        "# TODO Fix code duplication with plot_multi_model_single_feature_imp\n",
        "def plot_multi_model_feat_imps(model_vi, title, imp_col='imp', ft_mean_lim=10):\n",
        "  if title is None:\n",
        "    title = ''\n",
        "\n",
        "  n = 10\n",
        "  display(model_vi.sort_values(imp_col).tail(n))\n",
        "  display(model_vi.sort_values(imp_col).head(n))\n",
        "\n",
        "  model_vi.boxplot(imp_col)\n",
        "  plt.title(title + ' - Variable importance')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.boxplot(imp_col, by='type')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Feature types')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  bxpm = model_vi.boxplot(imp_col, by='model')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Models')\n",
        "  plt.suptitle('')\n",
        "  n = 2\n",
        "  plt.setp(bxpm.get_xticklabels()[::n], visible=False)\n",
        "  plt.show()\n",
        "\n",
        "  if np.min(model_vi['lag']) < 0:\n",
        "    bxpl = model_vi.loc[model_vi['lag'] < 0,].boxplot(imp_col, by='lag')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Past Lags')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if np.max(model_vi['lag']) > 0:\n",
        "    bxpl = model_vi.loc[model_vi['lag'] >= 0,].boxplot(imp_col, by='lag')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Future Lags')\n",
        "    plt.suptitle('')\n",
        "    n = 2\n",
        "    plt.setp(bxpl.get_xticklabels()[::n], visible=False)\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['window'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='window')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Windows')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['shift'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='shift')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Shifts')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  # fi_ft = groupby_multi_model_feat_imps(model_vi, 'feature_transform')\n",
        "  # print('fi_ft:')\n",
        "  # display(fi_ft)\n",
        "  # fts = fi_ft.loc[fi_ft['mean'] >= ft_mean_lim,].index.values.tolist()\n",
        "  # print(f'{fts = }')\n",
        "  #\n",
        "  # model_vi.loc[model_vi['feature_transform'].isin(fts),].boxplot(imp_col, by='feature_transform')\n",
        "  # plt.xticks(rotation=45, ha='right')\n",
        "  # plt.ylabel(str(imp_col))\n",
        "  # plt.title(title + ' - Feature transform mean >= ' + str(ft_mean_lim))\n",
        "  # plt.suptitle('')\n",
        "  # plt.show()\n",
        "\n",
        "  model_vi.boxplot(imp_col, by='feature_transform')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.title(title + ' - Feature Transform')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.loc[model_vi['above_cf'] == 1,].boxplot(imp_col, by='feature')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Features (above core features)')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.loc[model_vi['between_cf'] == 1,].boxplot(imp_col, by='feature')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Features (between core features)')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.loc[model_vi['above_cf'] == 1,].boxplot(imp_col, by='transform')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Transforms (above core features)')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.loc[model_vi['between_cf'] == 1,].boxplot(imp_col, by='transform')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Transforms (between core features)')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.boxplot(imp_col, by='feature')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.title(title + ' - Base Features')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.boxplot(imp_col, by='transform')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.title(title + ' - Transform')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_x_y_importance_interaction(model_vi, xvar, yvar, imp_var, title):\n",
        "  x = sorted(model_vi[xvar].unique())\n",
        "  y = sorted(model_vi[yvar].unique())\n",
        "  X, Y = np.meshgrid(x, y)\n",
        "\n",
        "  model_vi_grp_min  = model_vi[[yvar, xvar, imp_var]].groupby([yvar, xvar]).min(imp_var)\n",
        "  model_vi_grp_max  = model_vi[[yvar, xvar, imp_var]].groupby([yvar, xvar]).max(imp_var)\n",
        "  model_vi_grp_mean = model_vi[[yvar, xvar, imp_var]].groupby([yvar, xvar]).mean(imp_var)\n",
        "\n",
        "  zs_min  = np.array(model_vi_grp_min[imp_var])\n",
        "  zs_max  = np.array(model_vi_grp_max[imp_var])\n",
        "  zs_mean = np.array(model_vi_grp_mean[imp_var])\n",
        "  Zmin  = zs_min.reshape(X.shape)\n",
        "  Zmax  = zs_max.reshape(X.shape)\n",
        "  Zmean = zs_mean.reshape(X.shape)\n",
        "\n",
        "  fig = plt.figure(figsize=(14, 6))\n",
        "  ax1 = fig.add_subplot(131, projection='3d')\n",
        "  ax1.plot_surface(X, Y, Zmin, cmap='viridis')\n",
        "  ax1.set_xlabel(xvar.title())\n",
        "  ax1.set_ylabel(yvar.title())\n",
        "  ax1.set_title(title + ' - min')\n",
        "  # ax1.set_zlabel(str(imp_var).title())\n",
        "\n",
        "  ax2 = fig.add_subplot(132, projection='3d')\n",
        "  ax2.plot_surface(X, Y, Zmean, cmap='viridis')\n",
        "  ax2.set_xlabel(xvar.title())\n",
        "  ax2.set_ylabel(yvar.title())\n",
        "  ax2.set_title(title + ' - mean')\n",
        "  # ax2.set_zlabel(str(imp_var).title())\n",
        "\n",
        "  ax3 = fig.add_subplot(133, projection='3d')\n",
        "  ax3.plot_surface(X, Y, Zmax, cmap='viridis')\n",
        "  ax3.set_xlabel(xvar.title())\n",
        "  ax3.set_ylabel(yvar.title())\n",
        "  ax3.set_title(title + ' - max')\n",
        "  ax3.set_zlabel(str(imp_var).title())\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_multi_model_importance_interactions(model_vi, title=None):\n",
        "\n",
        "  plot_x_y_importance_interaction(model_vi, 'model', 'lag', 'imp', title)\n",
        "\n",
        "  if model_vi['window'].nunique() >= 2:\n",
        "    plot_x_y_importance_interaction(model_vi, 'model', 'window', 'imp', title)\n",
        "    plot_x_y_importance_interaction(model_vi.loc[model_vi['lag'] < 0,],\n",
        "                                    'lag',\n",
        "                                    'window',\n",
        "                                    'imp',\n",
        "                                    title)\n",
        "\n",
        "  if model_vi['shift'].nunique() >= 2:\n",
        "    plot_x_y_importance_interaction(model_vi, 'model', 'shift', 'imp', title)\n",
        "    plot_x_y_importance_interaction(model_vi.loc[model_vi['lag'] < 0,],\n",
        "                                    'lag',\n",
        "                                    'shift',\n",
        "                                    'imp',\n",
        "                                    title)\n",
        "\n",
        "\n",
        "def groupby_multi_model_feat_imps(mod_imps, group, verbose=False):\n",
        "  if verbose:\n",
        "    print('y_des_shadow:')\n",
        "    display(mod_imps.loc[mod_imps['feature'] == 'y_des_shadow', 'imp'].describe())\n",
        "\n",
        "  # mi_gb - model importance group by\n",
        "  mi_gb_desc = mod_imps[[group, 'imp']].groupby(group).describe()\n",
        "  mi_gb_desc = mi_gb_desc.droplevel(axis=1, level=0)\n",
        "  mi_gb_desc = mi_gb_desc.astype({'count': 'int', 'min': 'int', '25%': 'int',\n",
        "                                  '50%': 'int', '75%': 'int', 'max': 'int'})\n",
        "  if verbose:\n",
        "    display(mi_gb_desc)\n",
        "\n",
        "\n",
        "  mi_gb_sum = mod_imps[[group, 'imp']].groupby(group).sum()\n",
        "  mi_gb_sum = mi_gb_sum.rename(columns={'imp': 'sum'})\n",
        "  if verbose:\n",
        "    print('\\nsum:')\n",
        "    display(mi_gb_sum)\n",
        "\n",
        "  mi_gb_desc_sum = pd.merge(mi_gb_desc, mi_gb_sum, left_index=True, right_index=True)\n",
        "\n",
        "\n",
        "  mi_gb_0imp = mod_imps.loc[mod_imps['imp'] == 0].groupby(group).size().to_frame()\n",
        "  mi_gb_0imp = mi_gb_0imp.rename(columns={0: 'num_0'})\n",
        "  if verbose:\n",
        "    print('\\nnum 0 imp:')\n",
        "    display(mi_gb_0imp.sort_values('num_0'))\n",
        "\n",
        "\n",
        "  mi_gb_desc_sum_0imp = pd.merge(mi_gb_desc_sum, mi_gb_0imp, left_index=True, right_index=True)\n",
        "  mi_gb_desc_sum_0imp['pc_0'] = mi_gb_desc_sum_0imp['num_0'] * 100 / mi_gb_desc_sum_0imp['count']\n",
        "  # display(mi_gb_desc_sum_0imp)\n",
        "\n",
        "\n",
        "  mi_gb_shad_geq = mod_imps[[group, 'shadow_geq']].groupby(group).sum()\n",
        "  mi_gb_shad_geq = mi_gb_shad_geq.rename(columns={'shadow_geq': 'num_shad_geq'})\n",
        "  if verbose:\n",
        "    print('\\nnum_shad_geq:')\n",
        "    display(mi_gb_shad_geq)\n",
        "\n",
        "  mi_gb_all = pd.merge(mi_gb_desc_sum_0imp, mi_gb_shad_geq, left_index=True, right_index=True)\n",
        "  mi_gb_all['pc_shad_geq'] = mi_gb_all['num_shad_geq'] * 100 / mi_gb_all['count']\n",
        "  # display(mi_gb_all)\n",
        "\n",
        "  return mi_gb_all\n",
        "\n",
        "\n",
        "def plot_feat_imp_cumsum(mod_vi, title):\n",
        "  mod_impcs = mod_vi[['model', 'imp']].groupby('model').apply(lambda grp: grp.imp.sort_values(ascending=False).cumsum().reset_index(drop=True))\n",
        "  mod_impcs = mod_impcs.melt(ignore_index=False).reset_index().rename(columns={'imp': 'order', 'value': 'imp_cumsum'})\n",
        "\n",
        "  xvar = 'order'\n",
        "  yvar = 'model'\n",
        "  fig = plt.figure(figsize=(14, 6))\n",
        "  ax1 = fig.add_subplot(121, projection='3d')\n",
        "  ax1.plot_trisurf(mod_impcs[xvar], mod_impcs[yvar], mod_impcs['imp_cumsum'],\n",
        "                   cmap='viridis', linewidth=0)\n",
        "\n",
        "  ax1.set_xlabel(xvar.title())\n",
        "  ax1.set_ylabel(yvar.title())\n",
        "  ax1.set_title(title)\n",
        "\n",
        "  ax2 = fig.add_subplot(122, projection='3d')\n",
        "  ax2.plot_trisurf(mod_impcs[yvar], mod_impcs[xvar], mod_impcs['imp_cumsum'],\n",
        "                   cmap='viridis', linewidth=0)\n",
        "\n",
        "  ax2.set_xlabel(yvar.title())\n",
        "  ax2.set_ylabel(xvar.title())\n",
        "  ax2.set_zlabel('imp_cumsum')\n",
        "  ax2.set_title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# TODO Fix code duplication with plot_multi_model_feat_imps\n",
        "def plot_multi_model_single_feature_imp(model_vi, col_name, col_value, title, imp_col='imp'):\n",
        "  if title is None:\n",
        "    title = col_value\n",
        "  else:\n",
        "    title = title + ' ' + col_value\n",
        "\n",
        "  model_vi = model_vi.loc[model_vi[col_name] == col_value,]\n",
        "\n",
        "  model_vi.boxplot(imp_col)\n",
        "  plt.title(title + ' - Variable importance')\n",
        "  plt.show()\n",
        "\n",
        "  if model_vi['type'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='type')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Feature types')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  bxpm = model_vi.boxplot(imp_col, by='model')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Models')\n",
        "  plt.suptitle('')\n",
        "  n = 2\n",
        "  plt.setp(bxpm.get_xticklabels()[::n], visible=False)\n",
        "  plt.show()\n",
        "\n",
        "  # palette = ['red', 'green']\n",
        "  # box_cols = ['y_des_window_24_hist_mode', 'y_des_shadow']\n",
        "  # model3_vi_bp = model3_vi.loc[model3_vi['feature'].isin(box_cols)]\n",
        "  # sns.boxplot(data=model3_vi_bp, x='lag', y='imp', hue='feature',\n",
        "  #             gap=.2, palette=palette, fill=False, linewidth=.75)\n",
        "  # plt.show()\n",
        "\n",
        "  if np.min(model_vi['lag']) < 0:\n",
        "    bxpl = model_vi.loc[model_vi['lag'] < 0,].boxplot(imp_col, by='lag')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Past Lags')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if np.max(model_vi['lag']) > 0:\n",
        "    bxpl = model_vi.loc[model_vi['lag'] >= 0,].boxplot(imp_col, by='lag')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Future Lags')\n",
        "    plt.suptitle('')\n",
        "    n = 2\n",
        "    plt.setp(bxpl.get_xticklabels()[::n], visible=False)\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['window'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='window')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Windows')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['shift'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='shift')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Shifts')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['transform'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='transform')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Transforms')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  xvar = 'model'\n",
        "  yvar = 'lag'\n",
        "\n",
        "  fig = plt.figure(figsize=(14, 7))\n",
        "  ax1 = fig.add_subplot(121, projection='3d')\n",
        "  ax1.plot_trisurf(model_vi[xvar], model_vi[yvar], model_vi[imp_col],\n",
        "                   cmap='viridis', linewidth=0)\n",
        "  # ax1.plot_trisurf(model_vi[xvar], model_vi[yvar], model_vi['shadow_imp'],\n",
        "  #                  color='pink', linewidth=0)\n",
        "  ax1.set_xlabel(xvar.title())\n",
        "  ax1.set_ylabel(yvar.title())\n",
        "  ax1.set_title(title)\n",
        "\n",
        "  ax2 = fig.add_subplot(122, projection='3d')\n",
        "  ax2.plot_trisurf(model_vi[yvar], model_vi[xvar], model_vi[imp_col],\n",
        "                   cmap='viridis', linewidth=0)\n",
        "  # ax2.plot_trisurf(model_vi[yvar], model_vi[xvar], model_vi['shadow_imp'],\n",
        "  #                  color='pink', linewidth=0)\n",
        "  ax2.set_xlabel(yvar.title())\n",
        "  ax2.set_ylabel(xvar.title())\n",
        "  # ax2.zaxis.set_rotate_label(False)  # disable automatic rotation\n",
        "  ax2.set_zlabel(str(imp_col))  #, rotation=-90)\n",
        "  ax2.set_title(title)\n",
        "  plt.show()\n",
        "  print('\\n\\n')\n",
        "\n",
        "\n",
        "def load_features_file(feature_set,\n",
        "                       data_set,\n",
        "                       location = 'gdrive',\n",
        "                       date_str = '.2022.09.20',\n",
        "                       filex    = '.parquet'):\n",
        "\n",
        "  if location == 'github':\n",
        "    base_url = 'https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/data/features/'\n",
        "    filex += '?raw=true'\n",
        "  elif location == 'gdrive':\n",
        "    base_url = '/content/drive/MyDrive/data/CambridgeTemperatureNotebooks/features/'\n",
        "  else:\n",
        "    print(\"Unsupported 'location' in load_features_file function:\")\n",
        "    print('  location =', location)\n",
        "\n",
        "  file_str = feature_set + '_' + data_set + date_str + filex\n",
        "  data_url = base_url + file_str\n",
        "\n",
        "  df = pd.read_parquet(data_url)\n",
        "\n",
        "  df.set_index('ds', drop=False, inplace=True)\n",
        "  df = df[~df.index.duplicated(keep='first')]\n",
        "  df = df.asfreq(freq='30min')\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def load_train_valid_test_features(feature_set, location='gdrive'):\n",
        "  train = load_features_file(feature_set, 'train', location)\n",
        "  valid = load_features_file(feature_set, 'valid', location)\n",
        "  test  = load_features_file(feature_set, 'test',  location)\n",
        "\n",
        "  sanity_check_train_valid_test(train, valid, test)\n",
        "\n",
        "  check_high_low_thresholds(train, 'train '+feature_set)\n",
        "  check_high_low_thresholds(valid, 'valid '+feature_set)\n",
        "  check_high_low_thresholds(test,   'test '+feature_set)\n",
        "\n",
        "  return train, valid, test\n",
        "\n",
        "\n",
        "def get_feature_selection_data(df, sel_cols, y_col, fs_lags, pred_step):\n",
        "  feat_cols = df.columns.to_list()\n",
        "\n",
        "  excludes = ['_ucm_', '_yhat', '_diff_', '_yearly', '_daily', '_trend',\n",
        "              'humidity_des', 'pressure_des', 'ds', 'spike', 'tsclean',\n",
        "              'day.', 'year', 'rain', 'wind.', '.log', 'dy_dh', 'dy_dp',\n",
        "              'missing', 'dT_dH', 'dT_dP', 'dT_dTdp', 'known_inaccuracy',\n",
        "              'isd_', 'long_run', 'cooksd_out', 'tau'\n",
        "              ]\n",
        "  feat_cols = [feat_col for feat_col in feat_cols if all([x not in feat_col for x in excludes])]\n",
        "\n",
        "  feat_cols.extend(sel_cols)\n",
        "  feat_cols = list(set(feat_cols))\n",
        "\n",
        "  feat_cols.remove(y_col)\n",
        "  feat_cols.remove('y')\n",
        "  feat_cols.remove('dew.point')\n",
        "\n",
        "  all_cols = [*feat_cols, y_col]\n",
        "  df_nona = df[all_cols].dropna()\n",
        "\n",
        "  # Add lagged features\n",
        "  if fs_lags is not None:\n",
        "    # PerformanceWarning: DataFrame is highly fragmented ...\n",
        "    # df_nona_lagged = df_nona.assign(**{\n",
        "    #   f'{col}_lag_{lag}': df_nona[col].shift(lag)\n",
        "    #   for lag in fs_lags\n",
        "    #   for col in feat_cols})\n",
        "    df_nona_lagged_only = pd.DataFrame({\n",
        "       f'{col}_lag_{lag}': df_nona[col].shift(lag)\n",
        "       for lag in fs_lags\n",
        "       for col in feat_cols})\n",
        "    df_nona_lagged_plus = pd.concat([df_nona, df_nona_lagged_only], axis=1)\n",
        "    df_nona_lagged_plus = df_nona_lagged_plus.dropna()\n",
        "    X_df = df_nona_lagged_plus.drop(y_col, axis=1)\n",
        "    y_df = df_nona_lagged_plus[[y_col]]\n",
        "  else:\n",
        "    X_df = df_nona[feat_cols]\n",
        "    y_df = df_nona[[y_col]]\n",
        "\n",
        "  y_df = y_df[y_col].shift(-pred_step).dropna()\n",
        "  X_df = X_df.head(y_df.shape[0])\n",
        "\n",
        "  return X_df, y_df\n",
        "\n",
        "\n",
        "def get_feature_selection_scores(df, sel_cols, lags=None, pred_step=0,\n",
        "                                 y_col=Y_COL, sort_col='f_test', mi=False):\n",
        "  '''WARNING: These tests assume a linear model.  This may not be optimal.\n",
        "\n",
        "     Don't draw any hasty conclusions from these scores.\n",
        "  '''\n",
        "\n",
        "  X_df, y_df = get_feature_selection_data(df, sel_cols, y_col, lags, pred_step)\n",
        "  feat_cols = X_df.columns\n",
        "\n",
        "  f_tests, _ = f_regression(X_df, y_df)\n",
        "  f_tests /= np.sum(f_tests)\n",
        "\n",
        "  r_tests = r_regression(X_df, y_df)\n",
        "  r_tests /= np.sum(r_tests)\n",
        "\n",
        "  # Correlations with Y_COL\n",
        "  # corrs = []\n",
        "  # for feat in feat_cols:\n",
        "  #   corrs.append(X_df[feat].corr(y_df))\n",
        "\n",
        "  fs_df = pd.DataFrame({#'correlation': corrs,\n",
        "                        'r_test': r_tests.round(6),\n",
        "                        'f_test': f_tests.round(6),\n",
        "                       })\n",
        "  fs_df.index = feat_cols\n",
        "\n",
        "  # Slow :-(\n",
        "  if mi:\n",
        "    mi_feats = mutual_info_regression(X_df, y_df)\n",
        "    mi_feats /= np.sum(mi_feats)\n",
        "    fs_df['mi'] = mi_feats\n",
        "\n",
        "  if sort_col == 'f_test':\n",
        "    fs_df = fs_df.sort_values('f_test', ascending=False)\n",
        "  elif sort_col == 'r_test':\n",
        "    fs_df = fs_df.sort_values('r_test', ascending=False)\n",
        "  elif mi is True and sort_col == 'mi':\n",
        "    fs_df = fs_df.sort_values('mi', ascending=False)\n",
        "\n",
        "  return fs_df\n",
        "\n",
        "\n",
        "def get_above_between_below_features(fs, feats, feat_str, imp='f_test'):\n",
        "  # cf - core features\n",
        "  # sf - solar features\n",
        "\n",
        "  fs['above_' + feat_str] = 0\n",
        "  fs['below_' + feat_str] = 0\n",
        "\n",
        "  min_score = np.min(fs.loc[feats, imp])\n",
        "  max_score = np.max(fs.loc[feats, imp])\n",
        "\n",
        "  fs.loc[fs[imp] > max_score, 'above_' + feat_str] = 1\n",
        "  fs.loc[fs[imp] < min_score, 'below_' + feat_str] = 1\n",
        "\n",
        "  if len(feats) > 1:\n",
        "    fs['between_' + feat_str] = 0\n",
        "    fs.loc[(fs[imp] >= min_score) & (fs[imp] <= max_score), 'between_' + feat_str] = 1\n",
        "\n",
        "  return fs\n",
        "\n",
        "\n",
        "def get_multi_step_feat_sel_scores(train, sel_cols, lags=None, horizon=48):\n",
        "  fs_steps_list = []\n",
        "\n",
        "  for i in range(horizon):\n",
        "    fs_df = get_feature_selection_scores(train, sel_cols, lags, pred_step=i)\n",
        "    fs_df['rank'] = [i for i in range(fs_df.shape[0])]\n",
        "    fs_df['step'] = i\n",
        "\n",
        "    if lags is None:\n",
        "      fs_df['feature_transform'] = fs_df.index\n",
        "    else:\n",
        "      fs_df['feature_transform_lag'] = fs_df.index\n",
        "\n",
        "    fs_df.drop('r_test', axis=1, inplace=True)\n",
        "    fs_df = get_above_between_below_features(fs_df, ['dew.point_des', 'pressure', 'humidity'], 'cf')\n",
        "    fs_df = get_above_between_below_features(fs_df, ['irradiance', 'za_rad'], 'sf')\n",
        "    fs_df = get_above_between_below_features(fs_df, ['y_des_shadow'], 'shadow')\n",
        "    fs_steps_list.append(fs_df)\n",
        "\n",
        "  fs_steps = pd.concat(fs_steps_list)\n",
        "\n",
        "  # add window, shift, transform, feature ...\n",
        "\n",
        "  if lags is not None:\n",
        "    fs_steps['feature_transform'] = fs_steps['feature_transform_lag']\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "      fs_steps['feature_transform'] = fs_steps['feature_transform'].str.replace('_lag.*', '', regex=True)\n",
        "\n",
        "    fs_steps['lag'] = fs_steps['feature_transform_lag'].str.extract('_lag(-?\\d+)$').fillna(0).astype(int)\n",
        "    fs_feats = ['feature_transform_lag', 'feature_transform', 'feature',\n",
        "                'transform', 'window', 'shift', 'lag', 'step', 'f_test',\n",
        "                'rank', 'above_cf', 'between_cf', 'below_cf', 'above_sf',\n",
        "                'between_sf', 'below_sf', 'above_shadow', 'below_shadow']\n",
        "  else:\n",
        "    fs_steps['lag'] = 0\n",
        "    fs_feats = ['feature_transform', 'feature', 'transform', 'window', 'shift',\n",
        "                'lag', 'step', 'f_test', 'rank', 'above_cf', 'between_cf',\n",
        "                'below_cf', 'above_sf', 'between_sf', 'below_sf',\n",
        "                'above_shadow', 'below_shadow']\n",
        "\n",
        "  fs_steps['shift']  = fs_steps['feature_transform'].str.extract('_shift_(\\d+)_').fillna(0).astype(int)\n",
        "  fs_steps['window'] = fs_steps['feature_transform'].str.extract('_window_(\\d+)_').fillna(0).astype(int)\n",
        "\n",
        "  fs_steps['transform'] = fs_steps['feature_transform'].str.extract('_window_\\d+_([a-zA-Z0-9\\._]+)$').fillna('None')\n",
        "  # fs_steps['transform'] = fs_steps['transform'].str.replace('_lag_[0-9]+', '')\n",
        "\n",
        "  fs_steps.loc[fs_steps['transform'].str.match('intervals_intervals_mean'), 'transform'] = 'intervals_mean'\n",
        "\n",
        "  fs_steps.loc[fs_steps['feature_transform'].str.endswith('_shadow'), 'transform'] = 'shadow'\n",
        "  fs_steps.loc[fs_steps['feature_transform'].str.endswith('_grad'), 'transform'] = 'grad'\n",
        "  fs_steps.loc[fs_steps['feature_transform'].str.endswith('_des'), 'transform'] = 'des'\n",
        "\n",
        "  fs_steps['feature'] = fs_steps['feature_transform'].str.extract('([a-zA-Z0-9\\.]+)_').fillna('None')\n",
        "  fs_steps.loc[fs_steps['transform'] == 'None', 'feature'] = fs_steps.loc[fs_steps['transform'] == 'None', 'feature_transform']\n",
        "\n",
        "  fs_steps = fs_steps[fs_feats]\n",
        "\n",
        "  return fs_steps\n",
        "\n",
        "\n",
        "def groupby_multi_step_feat_sel_scores(mod_imps, group, verbose=False):\n",
        "  if verbose:\n",
        "    print('y_des_shadow:')\n",
        "    display(mod_imps.loc[mod_imps['feature_transform'] == 'y_des_shadow', 'f_test'].describe())\n",
        "\n",
        "  # mi_gb - model importance group by\n",
        "  mi_gb_desc = mod_imps[[group, 'f_test']].groupby(group).describe()\n",
        "  mi_gb_desc = mi_gb_desc.droplevel(axis=1, level=0)\n",
        "  mi_gb_desc = mi_gb_desc.astype({'count': 'int'})#, 'min': 'int', '25%': 'int',\n",
        "                                  #'50%': 'int', '75%': 'int', 'max': 'int'})\n",
        "  if verbose:\n",
        "    display(mi_gb_desc)\n",
        "\n",
        "\n",
        "  mi_gb_sum = mod_imps[[group, 'f_test']].groupby(group).sum()\n",
        "  mi_gb_sum = mi_gb_sum.rename(columns={'f_test': 'sum'})\n",
        "  if verbose:\n",
        "    print('\\nsum:')\n",
        "    display(mi_gb_sum)\n",
        "\n",
        "  mi_gb_desc_sum = pd.merge(mi_gb_desc, mi_gb_sum, left_index=True, right_index=True)\n",
        "\n",
        "\n",
        "  mi_gb_0imp = mod_imps.loc[mod_imps['f_test'] == 0.0].groupby(group).size().to_frame()\n",
        "  mi_gb_0imp = mi_gb_0imp.rename(columns={0: 'num_0'})\n",
        "  if verbose:\n",
        "    print('\\nnum 0 f_test:')\n",
        "    display(mi_gb_0imp.sort_values('num_0'))\n",
        "\n",
        "\n",
        "  mi_gb_desc_sum_0imp = pd.merge(mi_gb_desc_sum, mi_gb_0imp, how='outer', left_index=True, right_index=True)\n",
        "  mi_gb_desc_sum_0imp['num_0'] = mi_gb_desc_sum_0imp['num_0'].fillna(0)\n",
        "  mi_gb_desc_sum_0imp['pc_0'] = mi_gb_desc_sum_0imp['num_0'] * 100 / mi_gb_desc_sum_0imp['count']\n",
        "  # display(mi_gb_desc_sum_0imp)\n",
        "\n",
        "\n",
        "  #mi_gb_shad_geq = mod_imps[[group, 'shadow_geq']].groupby(group).sum()\n",
        "  #mi_gb_shad_geq = mi_gb_shad_geq.rename(columns={'shadow_geq': 'num_shad_geq'})\n",
        "  #if verbose:\n",
        "  #  print('\\nnum_shad_geq:')\n",
        "  #  display(mi_gb_shad_geq)\n",
        "\n",
        "  #mi_gb_all = pd.merge(mi_gb_desc_sum_0imp, mi_gb_shad_geq, left_index=True, right_index=True)\n",
        "  #mi_gb_all['pc_shad_geq'] = mi_gb_all['num_shad_geq'] * 100 / mi_gb_all['count']\n",
        "  # display(mi_gb_all)\n",
        "\n",
        "  #return mi_gb_all\n",
        "  return mi_gb_desc_sum_0imp\n",
        "\n",
        "\n",
        "def plot_multi_step_feat_sel_scores(model_vi, title, imp_col='f_test'):\n",
        "  if title is None:\n",
        "    title = ''\n",
        "\n",
        "  model_vi.boxplot(imp_col)\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Variable importance')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  bxpm = model_vi.boxplot(imp_col, by='step')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - forecast step')\n",
        "  plt.suptitle('')\n",
        "  n = 2\n",
        "  plt.setp(bxpm.get_xticklabels()[::n], visible=False)\n",
        "  plt.show()\n",
        "\n",
        "  if model_vi['window'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='window')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Windows')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['shift'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='shift')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Shifts')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  model_vi.loc[model_vi['above_cf'] == 1,].boxplot(imp_col, by='feature_transform')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Features (above core features)')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  # model_vi.boxplot(imp_col, by='feature_transform')\n",
        "  model_vi.loc[model_vi['between_cf'] == 1,].boxplot(imp_col, by='feature_transform')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Features (between core features)')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.boxplot(imp_col, by='feature')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Base Feature')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  model_vi.boxplot(imp_col, by='transform')\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "  plt.ylabel(str(imp_col))\n",
        "  plt.title(title + ' - Transform')\n",
        "  plt.suptitle('')\n",
        "  plt.show()\n",
        "\n",
        "  if model_vi['lag'].nunique() >= 2:\n",
        "    model_vi.boxplot(imp_col, by='lag')\n",
        "    plt.ylabel(str(imp_col))\n",
        "    plt.title(title + ' - Lag')\n",
        "    plt.suptitle('')\n",
        "    plt.show()\n",
        "\n",
        "  if model_vi['step'].nunique() >= 2 and model_vi['lag'].nunique() >= 2:\n",
        "    plot_x_y_importance_interaction(model_vi, 'step', 'lag', 'f_test', title)\n",
        "\n",
        "  if model_vi['window'].nunique() >= 2 and model_vi['lag'].nunique() >= 2:\n",
        "    plot_x_y_importance_interaction(model_vi, 'window', 'lag', 'f_test', title)\n",
        "\n",
        "  if model_vi['step'].nunique() >= 2 and model_vi['window'].nunique() >= 2:\n",
        "    plot_x_y_importance_interaction(model_vi, 'step', 'window', 'f_test', title)\n",
        "\n",
        "\n",
        "def summarise_multi_step_feat_sel_scores(train, title, sel_cols, fs_lags=None, ft_mean_lim=0.01, verbose=False):\n",
        "  fs = get_multi_step_feat_sel_scores(train, sel_cols, lags=fs_lags)\n",
        "\n",
        "  if verbose:\n",
        "    display(fs)\n",
        "\n",
        "  if fs_lags is not None:\n",
        "    print('feature_transform_lag - ft_mean_lim = ' + str(ft_mean_lim) + ':')\n",
        "    fs_ftl = groupby_multi_step_feat_sel_scores(fs, 'feature_transform_lag')\n",
        "    display(fs_ftl.loc[fs_ftl['mean'] >= ft_mean_lim,].sort_values('mean', ascending=False))\n",
        "    print('lag:')\n",
        "    fs_lag = groupby_multi_step_feat_sel_scores(fs, 'lag')\n",
        "    display(fs_lag.sort_values('mean', ascending=False))\n",
        "  else:\n",
        "    print('feature_transform - ft_mean_lim = ' + str(ft_mean_lim) + ':')\n",
        "    fs_ft = groupby_multi_step_feat_sel_scores(fs, 'feature_transform')\n",
        "    display(fs_ft.loc[fs_ft['mean'] >= ft_mean_lim,].sort_values('mean', ascending=False))\n",
        "\n",
        "  if verbose:\n",
        "    fs_feature = groupby_multi_step_feat_sel_scores(fs, 'feature')\n",
        "    print('feature:')\n",
        "    display(fs_feature)\n",
        "\n",
        "    fs_transform = groupby_multi_step_feat_sel_scores(fs, 'transform')\n",
        "    print('transform:')\n",
        "    display(fs_transform)\n",
        "\n",
        "    fs_window = groupby_multi_step_feat_sel_scores(fs, 'window')\n",
        "    print('window:')\n",
        "    display(fs_window)\n",
        "\n",
        "  plot_multi_step_feat_sel_scores(fs, title)\n",
        "\n",
        "\n",
        "def summarise_multi_model_feat_imps(model_vi, title, verbose=False):\n",
        "  if verbose:\n",
        "    display(model_vi)\n",
        "\n",
        "    for gb_col in ['model', 'lag', 'type', 'shift', 'window', 'feature', 'transform']:\n",
        "      if model_vi[gb_col].nunique() >= 2:\n",
        "        vi_by_gb = groupby_multi_model_feat_imps(model_vi, gb_col)\n",
        "        display(vi_by_gb); print()\n",
        "\n",
        "  plot_multi_model_feat_imps(model_vi, title)\n",
        "  plot_multi_model_importance_interactions(model_vi, title)\n",
        "\n",
        "  if verbose:\n",
        "    plot_feat_imp_cumsum(model_vi, title)"
      ],
      "metadata": {
        "id": "wor8bx41mytE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup\n",
        "\n",
        "<a name='import'></a>\n",
        "\n",
        "### Import Pre-calculated Features\n",
        "\n",
        "\n",
        "See [feature_engineering.ipynb](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/feature_engineering.ipynb) notebook for further details.\n",
        "\n",
        "Load default features:"
      ],
      "metadata": {
        "id": "ou6XPchynOId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, valid_df, test_df = load_train_valid_test_features('default', location='github')\n",
        "\n",
        "print('train_df:')\n",
        "print_df_summary(train_df)\n",
        "plot_cols = ['y', 'humidity', 'dew.point', 'wind.speed.mean.x',\n",
        "             'wind.speed.mean.y']  # 'pressure',\n",
        "plot_observation_examples(train_df, plot_cols)"
      ],
      "metadata": {
        "id": "EYryErUunbjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "...\n",
        "\n",
        "TODO Summarise results\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Comparison with Baselines\n",
        "\n",
        "Finally, we can compare the best performing gradient boosted etc models against the best baseline method.  The VAR (Vector Auto-Regression) model from the [baselines notebook](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/cammet_baselines_2021.ipynb) was the best performing baseline.\n",
        "\n",
        "The best encoder decoder model, after 5 training epochs, was conv2dk2d_28l_48s_16bs_448fm_64f_1ksf_7kst.  Here I train the same model for 20 epochs.\n",
        "\n",
        "Some points to note regarding the `plot_forecasts` diagnostic plot:\n",
        " * on validation data not test data\n",
        " * `plot_forecasts`\n",
        "   * plot example forecasts with observations and lagged temperatures\n",
        "      * first row shows examples of best near zero rmse forecasts\n",
        "      * second row shows examples of worst positive rmse forecasts\n",
        "      * third row shows examples of worst negative rmse forecasts\n",
        "      * lagged observations are negative\n",
        "      * the day of the year the forecast begins in and the rmse value is displayed above each sub-plot\n",
        "\n",
        "### Updated VAR model\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "AsS2y-8JkMk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxkFmWFNjNcr"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tools.eval_measures import rmse, medianabs\n",
        "\n",
        "\n",
        "def plot_baseline_metrics(metrics, main_title):\n",
        "  fig, axs = plt.subplots(1, 2, figsize = (14, 7))\n",
        "  fig.suptitle(main_title)\n",
        "  axs = axs.ravel()  # APL ftw!\n",
        "\n",
        "  methods = metrics.method.unique()\n",
        "\n",
        "  for method in methods:\n",
        "    met_df = metrics.query('metric == \"rmse\" & method == \"%s\"' % method)\n",
        "    axs[0].plot(met_df.horizon, met_df.value, color='blue', label='Updated VAR')\n",
        "\n",
        "  ivar_rmse = np.array([0.39, 0.52, 0.64, 0.75, 0.86, 0.96, 1.06, 1.15, 1.23,\n",
        "                        1.31, 1.38, 1.45, 1.51, 1.57, 1.63, 1.68, 1.73, 1.77,\n",
        "                        1.81, 1.85, 1.89, 1.92, 1.96, 1.99, 2.02, 2.05, 2.08,\n",
        "                        2.1 , 2.13, 2.15, 2.18, 2.2 , 2.22, 2.24, 2.26, 2.28,\n",
        "                        2.3 , 2.31, 2.33, 2.35, 2.36, 2.38, 2.39, 2.4 , 2.42,\n",
        "                        2.43, 2.44, 2.45])\n",
        "  steps = [i for i in range(1, len(ivar_rmse)+1)]\n",
        "  axs[0].plot(steps, ivar_rmse, color='black', label='Initial VAR')\n",
        "\n",
        "  axs[0].set_xlabel(\"horizon - half hour steps\")\n",
        "  axs[0].set_ylabel(\"rmse\")\n",
        "  # axs[0].legend(methods)\n",
        "\n",
        "\n",
        "  for method in methods:\n",
        "    met_df = metrics.query('metric == \"mae\" & method == \"%s\"' % method)\n",
        "    axs[1].plot(met_df.horizon, met_df.value, color='blue', label='Updated VAR')\n",
        "\n",
        "  ivar_mae = np.array([0.39, 0.49, 0.57, 0.66, 0.74, 0.83, 0.91, 0.98, 1.05,\n",
        "                       1.12, 1.18, 1.24, 1.29, 1.34, 1.39, 1.43, 1.47, 1.5 ,\n",
        "                       1.53, 1.56, 1.59, 1.62, 1.64, 1.66, 1.68, 1.7 , 1.72,\n",
        "                       1.73, 1.75, 1.76, 1.77, 1.78, 1.8 , 1.81, 1.82, 1.83,\n",
        "                       1.83, 1.84, 1.85, 1.85, 1.86, 1.86, 1.87, 1.87, 1.88,\n",
        "                       1.88, 1.89, 1.89])\n",
        "  axs[1].plot(steps, ivar_mae, color='black', label='Initial VAR')\n",
        "\n",
        "  axs[1].set_xlabel(\"horizon - half hour steps\")\n",
        "  axs[1].set_ylabel(\"mae\")\n",
        "  # axs[1].legend(methods)\n",
        "\n",
        "  plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"center left\", borderaxespad=0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def update_metrics(metrics, test_data, method, get_metrics,\n",
        "                   model = None,\n",
        "                   met_cols = ['type', 'method', 'metric', 'horizon', 'value']):\n",
        "  metrics_h = []\n",
        "\n",
        "  if method in ['SES', 'HWES']:\n",
        "    horizons = [i for i in range(4, 49, 4)]\n",
        "    horizons.insert(0, 1)\n",
        "  else:\n",
        "    # horizons = [1, 48]\n",
        "    horizons = range(1, 49)\n",
        "\n",
        "  if method in ['VAR']:\n",
        "    variates = 'multivariate'\n",
        "  else:\n",
        "    variates = 'univariate'\n",
        "\n",
        "  print(\"h\\trmse\\tmae\")\n",
        "  for h in horizons:\n",
        "    if method in ['VAR']:\n",
        "      rmse_h, mae_h = get_metrics(test_data, h, method, model)\n",
        "    else:\n",
        "      rmse_h, mae_h = get_metrics(test_data, h, method)\n",
        "\n",
        "    metrics_h.append(dict(zip(met_cols, [variates, method, 'rmse', h, rmse_h])))\n",
        "    metrics_h.append(dict(zip(met_cols, [variates, method,  'mae', h,  mae_h])))\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  metrics_method = pd.DataFrame(metrics_h, columns = met_cols)\n",
        "  metrics = metrics.append(metrics_method)\n",
        "\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# rolling_cv with pre-trained model\n",
        "def var_rolling_cv(data, horizon, method, model):\n",
        "    lags = model.k_ar  # lag order\n",
        "    i = lags\n",
        "    h = horizon\n",
        "    rmse_roll, mae_roll = [], []\n",
        "    endo_vars = ['y', 'dew.point', 'humidity', 'pressure']\n",
        "    exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "                 'irradiance', 'azimuth_cos', 'za_rad'\n",
        "                ]\n",
        "\n",
        "    while (i + h) < len(data):\n",
        "        obs_df  = data[endo_vars].iloc[i:(i + h)]\n",
        "        endo_df = data[endo_vars].iloc[(i - lags):i].values\n",
        "        exog_df = data[exog_vars].iloc[i:(i + h)]\n",
        "\n",
        "        # y_hat = model.forecast(endo_df, steps = h)\n",
        "        y_hat = model.forecast(endo_df, exog_future = exog_df, steps = h)\n",
        "        preds = pd.DataFrame(y_hat, columns = endo_vars)\n",
        "\n",
        "        rmse_i = rmse(obs_df.y,      preds.y)\n",
        "        mae_i  = medianabs(obs_df.y, preds.y)\n",
        "        rmse_roll.append(rmse_i)\n",
        "        mae_roll.append(mae_i)\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "    print(h, '\\t', np.nanmean(rmse_roll).round(3), '\\t', np.nanmean(mae_roll).round(3))\n",
        "\n",
        "    return [np.nanmean(rmse_roll).round(2), np.nanmean(mae_roll).round(2)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DsHIe7adkVx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# approx. 5 mins\n",
        "\n",
        "train_df = train_df.asfreq(freq='30min')\n",
        "valid_df = valid_df.asfreq(freq='30min')\n",
        "test_df  = test_df.asfreq(freq='30min')\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "\n",
        "endo_vars = ['y', 'dew.point', 'humidity', 'pressure']\n",
        "exog_vars = [\n",
        "            'day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "            'irradiance', 'azimuth_cos', 'za_rad'\n",
        "            ]\n",
        "endo_df = train_df[endo_vars]\n",
        "exog_df = train_df[exog_vars]\n",
        "\n",
        "var_model = VAR(endog = endo_df, exog = exog_df)\n",
        "# var_model = VAR(endog = endo_df)\n",
        "MAX_LAGS = 96\n",
        "lag_order_res = var_model.select_order(MAX_LAGS)\n",
        "display(lag_order_res.summary())\n",
        "display(lag_order_res.selected_orders)\n",
        "print(lag_order_res.selected_orders['bic'])\n",
        "\n",
        "lag_order_table = lag_order_res.summary().data\n",
        "headers = lag_order_table.pop(0)\n",
        "lag_order_df = pd.DataFrame(lag_order_table, columns=headers)\n",
        "lag_order_df.drop('', axis=1, inplace=True)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    lag_order_df = pd.concat([lag_order_df[col].str.replace('*', '').astype(float)\n",
        "                             for col in lag_order_df], axis=1)\n",
        "\n",
        "lag_order_df.loc[1:, ['AIC','BIC','HQIC']].plot()\n",
        "plt.xlabel('lag')\n",
        "plt.ylabel('IC')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mScNU7akW7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lowest BIC value occurs at 51 lags.  I'm going to use `maxlags = 51` because that is where decreasing returns sets in."
      ],
      "metadata": {
        "id": "eXY9mITnkk7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_var_backtest(model, data, endo_vars, exog_vars, y_col=Y_COL, horizon=HORIZON):\n",
        "  lags = model.k_ar  # lag order\n",
        "  i = lags\n",
        "  h = horizon\n",
        "  preds = []\n",
        "\n",
        "  while (i + h) < len(data):\n",
        "    if i % 1000 == 0:\n",
        "      print(i)\n",
        "\n",
        "    obs_df  = data[endo_vars].iloc[i:(i + h)]\n",
        "    endo_vals = data[endo_vars].iloc[(i - lags):i].values\n",
        "\n",
        "    if exog_vars is not None:\n",
        "      exog_df = data[exog_vars].iloc[i:(i + h)]\n",
        "      y_hat_lol = model.forecast(endo_vals, exog_future = exog_df, steps = h)\n",
        "    else:\n",
        "      y_hat_lol = model.forecast(endo_vals, steps = h)\n",
        "\n",
        "    y_col_pos = 0  # hardcoding is bad mkay - make function param?\n",
        "    y_hat_series = pd.Series(data  = [y_hat_l[y_col_pos] for y_hat_l in y_hat_lol],\n",
        "                             index = obs_df.index,\n",
        "                             name  = y_col)\n",
        "    y_hat_ts = TimeSeries.from_series(y_hat_series)\n",
        "    # y_hat_ts = TimeSeries.from_values(np.array([y_hat_l[y_col_pos] for y_hat_l in y_hat_lol]))\n",
        "    # y_hat = [y_hat_l[y_col_pos] for y_hat_l in y_hat_lol]\n",
        "\n",
        "    preds.append(y_hat_ts)\n",
        "    i = i + 1\n",
        "\n",
        "  return preds\n",
        "\n",
        "\n",
        "var_fit = var_model.fit(maxlags = 51, ic = 'bic')\n",
        "print(var_fit.summary())\n",
        "\n",
        "main_var_col = 'y'\n",
        "backtest_var = get_var_backtest(var_fit, valid_df, endo_vars, exog_vars, y_col = main_var_col)\n",
        "# display(len(backtest_var))\n",
        "# display(backtest_var[0])\n",
        "hist_comp_var = get_historic_comparison(backtest_var, valid_df, y_col = main_var_col)\n",
        "# display(hist_comp_var)\n",
        "summarise_historic_comparison(hist_comp_var, valid_df, y_col = main_var_col)\n",
        "\n",
        "title_var = 'VAR ' + main_var_col + '...'\n",
        "plot_multistep_diagnostics(hist_comp_var, title_var, y_col = main_var_col)\n",
        "\n",
        "\n",
        "# metric_cols = ['type', 'method', 'metric', 'horizon', 'value']\n",
        "# metrics = pd.DataFrame([], columns = metric_cols)\n",
        "# metrics = update_metrics(metrics, valid_df, 'VAR', var_rolling_cv, var_fit)\n",
        "## metrics = update_metrics(metrics, test_df, 'VAR', var_rolling_cv, var_fit)\n",
        "# plot_baseline_metrics(metrics, 'Multivariate Baseline Comparison - 2021 valid data')\n",
        "\n",
        "\n",
        "# 2019 data\n",
        "# maxlags = 5\n",
        "# ...\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.39 \t 0.39\n",
        "# 48 \t 2.45 \t 1.89\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# maxlags = 52\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.37 \t 0.37\n",
        "# 48 \t 2.253 \t 1.784\n",
        "# maxlags = 52 substantially better than maxlags = 9\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'humidity',]\n",
        "# maxlags = 52\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.37 \t 0.37\n",
        "# 48 \t 2.293 \t 1.814\n",
        "# including pressure is beneficial\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['za_rad', 'irradiance', 'azimuth_cos',]\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.369 \t 0.369\n",
        "# 48   2.163 \t 1.729\n",
        "# exog_vars is beneficial\n",
        "# 1 hr 28 mins :-(\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',]\n",
        "# maxlags = 52\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.37 \t 0.37\n",
        "# 48   2.133 \t 1.68\n",
        "# Sinusoidal terms better than irradiance etc!\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1', 'irradiance']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.369 \t 0.369\n",
        "# 48   2.105 \t 1.667\n",
        "# irradiance worth adding to sinusoidal terms\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1', 'za_rad']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.37 \t 0.37\n",
        "# 48   2.134 \t 1.679\n",
        "# za_rad not as beneficial as irradiance\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1', 'azimuth_cos']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1    0.37 \t 0.37\n",
        "# 48   2.131 \t 1.675\n",
        "# azimuth_cos more beneficial than za_rad\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.368 \t 0.368\n",
        "# 48 \t 2.098 \t 1.658\n",
        "# Best model so far\n",
        "\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.368 \t 0.368\n",
        "# 48 \t 2.098 \t 1.657\n",
        "# Marginally better with za_rad\n",
        "\n",
        "# valid_df\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 51\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.347 \t 0.347\n",
        "# 48 \t 2.012 \t 1.581\n",
        "#\n",
        "\n",
        "# valid_df\n",
        "# endo_vars = ['y_des', 'dew.point_des', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 53\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.347 \t 0.347\n",
        "# 48 \t 2.724   2.132\n",
        "\n",
        "# valid_df\n",
        "# endo_vars = ['y_des', 'dew.point_des', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 53\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.357 \t 0.357\n",
        "# 48 \t 2.712   2.121\n",
        "\n",
        "# valid_df\n",
        "# train_df.loc['2016-01-01':,]\n",
        "# endo_vars = ['y', 'dew.point', 'pressure', 'humidity',]\n",
        "# exog_vars = ['day.cos.1', 'day.sin.1', 'year.cos.1', 'year.sin.1',\n",
        "#              'irradiance', 'azimuth_cos', 'za_rad']\n",
        "# maxlags = 22\n",
        "# h\t   rmse\t   mae\n",
        "# 1 \t 0.352 \t 0.352\n",
        "# 48 \t 2.926   2.305\n",
        "# Backtest RMSE 48th: 2.92592\n",
        "# Backtest MAE 48th:  2.304481\n",
        "# Radical decrease in maxlags!\n",
        "# Not a great model\n"
      ],
      "metadata": {
        "id": "JKNazIIokpEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rmse\n",
        "```\n",
        "[0.39, 0.52, 0.64, 0.75, 0.86, 0.96, 1.06, 1.15, 1.23,\n",
        " 1.31, 1.38, 1.45, 1.51, 1.57, 1.63, 1.68, 1.73, 1.77,\n",
        " 1.81, 1.85, 1.89, 1.92, 1.96, 1.99, 2.02, 2.05, 2.08,\n",
        " 2.1 , 2.13, 2.15, 2.18, 2.2 , 2.22, 2.24, 2.26, 2.28,\n",
        " 2.3 , 2.31, 2.33, 2.35, 2.36, 2.38, 2.39, 2.4 , 2.42,\n",
        " 2.43, 2.44, 2.45]\n",
        "```\n",
        "\n",
        "mae\n",
        "```\n",
        "[0.39, 0.49, 0.57, 0.66, 0.74, 0.83, 0.91, 0.98, 1.05,\n",
        " 1.12, 1.18, 1.24, 1.29, 1.34, 1.39, 1.43, 1.47, 1.5 ,\n",
        " 1.53, 1.56, 1.59, 1.62, 1.64, 1.66, 1.68, 1.7 , 1.72,\n",
        " 1.73, 1.75, 1.76, 1.77, 1.78, 1.8 , 1.81, 1.82, 1.83,\n",
        " 1.83, 1.84, 1.85, 1.85, 1.86, 1.86, 1.87, 1.87, 1.88,\n",
        " 1.88, 1.89, 1.89]\n",
        "```"
      ],
      "metadata": {
        "id": "4Q8X3C6Rk0s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_fit.plot()\n",
        "plt.show()\n",
        "\n",
        "# var_fit.plot_acorr()\n",
        "# plt.show()\n",
        "\n",
        "var_fit.fevd(48).plot()\n",
        "plt.show()\n",
        "\n",
        "var_fit.mse(48)"
      ],
      "metadata": {
        "id": "uNhTtXq6k6Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The updated VAR model shows substantial improvement.  It would benefit from further validation, including residual plots, QQ plots, autocorrelation of residual plots, residual boxplots across the forecast horizon steps etc\n",
        "\n",
        "NOTE: Updated VAR validated on 2021 data; initial VAR validated on 2019 data.\n",
        "\n",
        "TODO Move VAR baseline to separate notebook\n",
        "\n",
        "---\n",
        "\n",
        "**TODO** Plot model diagnostics.\n",
        "\n",
        "\n",
        "Next, I plot the best model and VAR model rmse and mae values for forecast horizons up to 48 (24 hours, each horizon step is equivalent to 30 minutes).  This plot plus the two others are for forecasts on the previously unused 2019 \"test\" data.  This is different from the 2018 \"validation\" data used elsewhere in this notebook.\n",
        "\n",
        "Some points to note regarding diagnostic plots:\n",
        " * once again, on test data not validation data\n",
        " * `plot_horizon_metrics`\n",
        "   * plot rmse and mae values for each individual step-ahead\n",
        " * `check_residuals`\n",
        "   * observations against predictions\n",
        "   * residuals over time\n",
        "   * residual distribution\n",
        " * `plot_forecasts`\n",
        "   * see sub-section immediately above for notable points"
      ],
      "metadata": {
        "id": "iOtK1fwHlJkw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrDh_OIHlL8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadly speaking, these results are very similar to the results from the VAR model.\n",
        "\n",
        "\n",
        "Diagnostic plots summary:\n",
        " * once again, these plots use test data not validation data\n",
        " * `plot_horizon_metrics`\n",
        "   * initially, these results look quite contradictory\n",
        "   * the rmse plot indicates better forecasts for the VAR method (in orange)\n",
        "   * the mae plot indicates better forecasts for the Conv2D kernel 2D method (in blue, mis-labelled as LSTM)\n",
        " * `check_residuals`\n",
        "   * the observations against predictions plot indicates\n",
        "     * predictions are too high at cold temperatures (below 0 C)\n",
        "     * predictions are too low at hot temperatures (above 25 C)\n",
        "   * residuals over time\n",
        "     * no obvious heteroscadicity\n",
        "     * no obvious periodicity\n",
        "       * surprising given observations against predictions plot\n",
        "   * residual distribution appears to be approximately normal (slightly right-skewed)\n",
        "     * no obvious sign of fat tails\n",
        " * `plot_forecasts`\n",
        "   * notable lack of noisy observations for the large positive and negative rmse examples\n",
        "\n",
        "The median absolute error (mae) is less sensitive to outliers compared to the root mean squared error (rmse) metric.\n",
        "\n",
        "Therefore, the rmse and mae plot difference may be due to the presence of outliers. I have maintained from the start that this data set is quite noisy, and attempts to correct these problems may have unintensionally introduced new issues.\n",
        "\n",
        "Transformed mean values across the 48 step horizon:\n",
        " * rmse of 2.05796\n",
        " * mae of 1.17986\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The best results from the gradient boosted trees are similar/different to  results from the [best LSTM model](https://github.com/makeyourownmaker/CambridgeTemperatureNotebooks/blob/main/notebooks/lstm_time_series.ipynb).\n",
        "\n",
        "How and why are they similar/different?\n",
        "\n",
        "...\n",
        "\n",
        "The conclusion is separated into the following sections:\n",
        " 1. What worked\n",
        " 2. What didn't work\n",
        " 3. Rejected ideas\n",
        " 4. Future work\n"
      ],
      "metadata": {
        "id": "OvixNo_llMq-"
      }
    }
  ]
}